{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of possible chords "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the number of possible chords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jl_dictionaries import Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C3',\n",
       " 'C5',\n",
       " 'C',\n",
       " 'Cm',\n",
       " 'Cdim',\n",
       " 'Caug',\n",
       " 'Csus2',\n",
       " 'Csus4',\n",
       " 'C6',\n",
       " 'C7',\n",
       " 'C7-5',\n",
       " 'Cmaj7b5',\n",
       " 'C7+5',\n",
       " 'C6sus2',\n",
       " 'C7sus2',\n",
       " 'C7sus4',\n",
       " 'Cm6',\n",
       " 'Cm7',\n",
       " 'CAmMaj7',\n",
       " 'Cm7b5',\n",
       " 'Cdim6',\n",
       " 'CM7',\n",
       " 'CM7+5',\n",
       " 'Cadd9',\n",
       " 'Cmadd9',\n",
       " 'Cadd2',\n",
       " 'Cadd4',\n",
       " 'Cadd11',\n",
       " 'Csus4(add13)',\n",
       " 'C6/9',\n",
       " 'C6add11',\n",
       " 'Cm6add11',\n",
       " 'Cadd4add9',\n",
       " 'C9',\n",
       " 'Cm9',\n",
       " 'CM9',\n",
       " 'C9sus4',\n",
       " 'C7_6',\n",
       " 'C7-9',\n",
       " 'C7+9',\n",
       " 'C9-5',\n",
       " 'C9+5',\n",
       " 'C7#9b5',\n",
       " 'C7#9#5',\n",
       " 'C7b9b5',\n",
       " 'C7b9#5',\n",
       " 'C11',\n",
       " 'C7+11',\n",
       " 'Cm7add11',\n",
       " 'Cmaj11',\n",
       " 'Cm11',\n",
       " 'C7b9#9',\n",
       " 'C7b9#11',\n",
       " 'C7#9#11',\n",
       " 'C7-13',\n",
       " 'Cm7add4',\n",
       " 'C9+11',\n",
       " 'Cm9+11',\n",
       " 'C13',\n",
       " 'Cmaj13',\n",
       " 'Cmadd13',\n",
       " 'C13sus4',\n",
       " 'C13-9',\n",
       " 'C13+9',\n",
       " 'C13+11',\n",
       " 'Cm13+11',\n",
       " 'Cmadd11',\n",
       " 'Db3',\n",
       " 'Db5',\n",
       " 'Db',\n",
       " 'Dbm',\n",
       " 'Dbdim',\n",
       " 'Dbaug',\n",
       " 'Dbsus2',\n",
       " 'Dbsus4',\n",
       " 'Db6',\n",
       " 'Db7',\n",
       " 'Db7-5',\n",
       " 'Dbmaj7b5',\n",
       " 'Db7+5',\n",
       " 'Db6sus2',\n",
       " 'Db7sus2',\n",
       " 'Db7sus4',\n",
       " 'Dbm6',\n",
       " 'Dbm7',\n",
       " 'DbAmMaj7',\n",
       " 'Dbm7b5',\n",
       " 'Dbdim6',\n",
       " 'DbM7',\n",
       " 'DbM7+5',\n",
       " 'Dbadd9',\n",
       " 'Dbmadd9',\n",
       " 'Dbadd2',\n",
       " 'Dbadd4',\n",
       " 'Dbadd11',\n",
       " 'Dbsus4(add13)',\n",
       " 'Db6/9',\n",
       " 'Db6add11',\n",
       " 'Dbm6add11',\n",
       " 'Dbadd4add9',\n",
       " 'Db9',\n",
       " 'Dbm9',\n",
       " 'DbM9',\n",
       " 'Db9sus4',\n",
       " 'Db7_6',\n",
       " 'Db7-9',\n",
       " 'Db7+9',\n",
       " 'Db9-5',\n",
       " 'Db9+5',\n",
       " 'Db7#9b5',\n",
       " 'Db7#9#5',\n",
       " 'Db7b9b5',\n",
       " 'Db7b9#5',\n",
       " 'Db11',\n",
       " 'Db7+11',\n",
       " 'Dbm7add11',\n",
       " 'Dbmaj11',\n",
       " 'Dbm11',\n",
       " 'Db7b9#9',\n",
       " 'Db7b9#11',\n",
       " 'Db7#9#11',\n",
       " 'Db7-13',\n",
       " 'Dbm7add4',\n",
       " 'Db9+11',\n",
       " 'Dbm9+11',\n",
       " 'Db13',\n",
       " 'Dbmaj13',\n",
       " 'Dbmadd13',\n",
       " 'Db13sus4',\n",
       " 'Db13-9',\n",
       " 'Db13+9',\n",
       " 'Db13+11',\n",
       " 'Dbm13+11',\n",
       " 'Dbmadd11',\n",
       " 'D3',\n",
       " 'D5',\n",
       " 'D',\n",
       " 'Dm',\n",
       " 'Ddim',\n",
       " 'Daug',\n",
       " 'Dsus2',\n",
       " 'Dsus4',\n",
       " 'D6',\n",
       " 'D7',\n",
       " 'D7-5',\n",
       " 'Dmaj7b5',\n",
       " 'D7+5',\n",
       " 'D6sus2',\n",
       " 'D7sus2',\n",
       " 'D7sus4',\n",
       " 'Dm6',\n",
       " 'Dm7',\n",
       " 'DAmMaj7',\n",
       " 'Dm7b5',\n",
       " 'Ddim6',\n",
       " 'DM7',\n",
       " 'DM7+5',\n",
       " 'Dadd9',\n",
       " 'Dmadd9',\n",
       " 'Dadd2',\n",
       " 'Dadd4',\n",
       " 'Dadd11',\n",
       " 'Dsus4(add13)',\n",
       " 'D6/9',\n",
       " 'D6add11',\n",
       " 'Dm6add11',\n",
       " 'Dadd4add9',\n",
       " 'D9',\n",
       " 'Dm9',\n",
       " 'DM9',\n",
       " 'D9sus4',\n",
       " 'D7_6',\n",
       " 'D7-9',\n",
       " 'D7+9',\n",
       " 'D9-5',\n",
       " 'D9+5',\n",
       " 'D7#9b5',\n",
       " 'D7#9#5',\n",
       " 'D7b9b5',\n",
       " 'D7b9#5',\n",
       " 'D11',\n",
       " 'D7+11',\n",
       " 'Dm7add11',\n",
       " 'Dmaj11',\n",
       " 'Dm11',\n",
       " 'D7b9#9',\n",
       " 'D7b9#11',\n",
       " 'D7#9#11',\n",
       " 'D7-13',\n",
       " 'Dm7add4',\n",
       " 'D9+11',\n",
       " 'Dm9+11',\n",
       " 'D13',\n",
       " 'Dmaj13',\n",
       " 'Dmadd13',\n",
       " 'D13sus4',\n",
       " 'D13-9',\n",
       " 'D13+9',\n",
       " 'D13+11',\n",
       " 'Dm13+11',\n",
       " 'Dmadd11',\n",
       " 'Eb3',\n",
       " 'Eb5',\n",
       " 'Eb',\n",
       " 'Ebm',\n",
       " 'Ebdim',\n",
       " 'Ebaug',\n",
       " 'Ebsus2',\n",
       " 'Ebsus4',\n",
       " 'Eb6',\n",
       " 'Eb7',\n",
       " 'Eb7-5',\n",
       " 'Ebmaj7b5',\n",
       " 'Eb7+5',\n",
       " 'Eb6sus2',\n",
       " 'Eb7sus2',\n",
       " 'Eb7sus4',\n",
       " 'Ebm6',\n",
       " 'Ebm7',\n",
       " 'EbAmMaj7',\n",
       " 'Ebm7b5',\n",
       " 'Ebdim6',\n",
       " 'EbM7',\n",
       " 'EbM7+5',\n",
       " 'Ebadd9',\n",
       " 'Ebmadd9',\n",
       " 'Ebadd2',\n",
       " 'Ebadd4',\n",
       " 'Ebadd11',\n",
       " 'Ebsus4(add13)',\n",
       " 'Eb6/9',\n",
       " 'Eb6add11',\n",
       " 'Ebm6add11',\n",
       " 'Ebadd4add9',\n",
       " 'Eb9',\n",
       " 'Ebm9',\n",
       " 'EbM9',\n",
       " 'Eb9sus4',\n",
       " 'Eb7_6',\n",
       " 'Eb7-9',\n",
       " 'Eb7+9',\n",
       " 'Eb9-5',\n",
       " 'Eb9+5',\n",
       " 'Eb7#9b5',\n",
       " 'Eb7#9#5',\n",
       " 'Eb7b9b5',\n",
       " 'Eb7b9#5',\n",
       " 'Eb11',\n",
       " 'Eb7+11',\n",
       " 'Ebm7add11',\n",
       " 'Ebmaj11',\n",
       " 'Ebm11',\n",
       " 'Eb7b9#9',\n",
       " 'Eb7b9#11',\n",
       " 'Eb7#9#11',\n",
       " 'Eb7-13',\n",
       " 'Ebm7add4',\n",
       " 'Eb9+11',\n",
       " 'Ebm9+11',\n",
       " 'Eb13',\n",
       " 'Ebmaj13',\n",
       " 'Ebmadd13',\n",
       " 'Eb13sus4',\n",
       " 'Eb13-9',\n",
       " 'Eb13+9',\n",
       " 'Eb13+11',\n",
       " 'Ebm13+11',\n",
       " 'Ebmadd11',\n",
       " 'E3',\n",
       " 'E5',\n",
       " 'E',\n",
       " 'Em',\n",
       " 'Edim',\n",
       " 'Eaug',\n",
       " 'Esus2',\n",
       " 'Esus4',\n",
       " 'E6',\n",
       " 'E7',\n",
       " 'E7-5',\n",
       " 'Emaj7b5',\n",
       " 'E7+5',\n",
       " 'E6sus2',\n",
       " 'E7sus2',\n",
       " 'E7sus4',\n",
       " 'Em6',\n",
       " 'Em7',\n",
       " 'EAmMaj7',\n",
       " 'Em7b5',\n",
       " 'Edim6',\n",
       " 'EM7',\n",
       " 'EM7+5',\n",
       " 'Eadd9',\n",
       " 'Emadd9',\n",
       " 'Eadd2',\n",
       " 'Eadd4',\n",
       " 'Eadd11',\n",
       " 'Esus4(add13)',\n",
       " 'E6/9',\n",
       " 'E6add11',\n",
       " 'Em6add11',\n",
       " 'Eadd4add9',\n",
       " 'E9',\n",
       " 'Em9',\n",
       " 'EM9',\n",
       " 'E9sus4',\n",
       " 'E7_6',\n",
       " 'E7-9',\n",
       " 'E7+9',\n",
       " 'E9-5',\n",
       " 'E9+5',\n",
       " 'E7#9b5',\n",
       " 'E7#9#5',\n",
       " 'E7b9b5',\n",
       " 'E7b9#5',\n",
       " 'E11',\n",
       " 'E7+11',\n",
       " 'Em7add11',\n",
       " 'Emaj11',\n",
       " 'Em11',\n",
       " 'E7b9#9',\n",
       " 'E7b9#11',\n",
       " 'E7#9#11',\n",
       " 'E7-13',\n",
       " 'Em7add4',\n",
       " 'E9+11',\n",
       " 'Em9+11',\n",
       " 'E13',\n",
       " 'Emaj13',\n",
       " 'Emadd13',\n",
       " 'E13sus4',\n",
       " 'E13-9',\n",
       " 'E13+9',\n",
       " 'E13+11',\n",
       " 'Em13+11',\n",
       " 'Emadd11',\n",
       " 'F3',\n",
       " 'F5',\n",
       " 'F',\n",
       " 'Fm',\n",
       " 'Fdim',\n",
       " 'Faug',\n",
       " 'Fsus2',\n",
       " 'Fsus4',\n",
       " 'F6',\n",
       " 'F7',\n",
       " 'F7-5',\n",
       " 'Fmaj7b5',\n",
       " 'F7+5',\n",
       " 'F6sus2',\n",
       " 'F7sus2',\n",
       " 'F7sus4',\n",
       " 'Fm6',\n",
       " 'Fm7',\n",
       " 'FAmMaj7',\n",
       " 'Fm7b5',\n",
       " 'Fdim6',\n",
       " 'FM7',\n",
       " 'FM7+5',\n",
       " 'Fadd9',\n",
       " 'Fmadd9',\n",
       " 'Fadd2',\n",
       " 'Fadd4',\n",
       " 'Fadd11',\n",
       " 'Fsus4(add13)',\n",
       " 'F6/9',\n",
       " 'F6add11',\n",
       " 'Fm6add11',\n",
       " 'Fadd4add9',\n",
       " 'F9',\n",
       " 'Fm9',\n",
       " 'FM9',\n",
       " 'F9sus4',\n",
       " 'F7_6',\n",
       " 'F7-9',\n",
       " 'F7+9',\n",
       " 'F9-5',\n",
       " 'F9+5',\n",
       " 'F7#9b5',\n",
       " 'F7#9#5',\n",
       " 'F7b9b5',\n",
       " 'F7b9#5',\n",
       " 'F11',\n",
       " 'F7+11',\n",
       " 'Fm7add11',\n",
       " 'Fmaj11',\n",
       " 'Fm11',\n",
       " 'F7b9#9',\n",
       " 'F7b9#11',\n",
       " 'F7#9#11',\n",
       " 'F7-13',\n",
       " 'Fm7add4',\n",
       " 'F9+11',\n",
       " 'Fm9+11',\n",
       " 'F13',\n",
       " 'Fmaj13',\n",
       " 'Fmadd13',\n",
       " 'F13sus4',\n",
       " 'F13-9',\n",
       " 'F13+9',\n",
       " 'F13+11',\n",
       " 'Fm13+11',\n",
       " 'Fmadd11',\n",
       " 'F#3',\n",
       " 'F#5',\n",
       " 'F#',\n",
       " 'F#m',\n",
       " 'F#dim',\n",
       " 'F#aug',\n",
       " 'F#sus2',\n",
       " 'F#sus4',\n",
       " 'F#6',\n",
       " 'F#7',\n",
       " 'F#7-5',\n",
       " 'F#maj7b5',\n",
       " 'F#7+5',\n",
       " 'F#6sus2',\n",
       " 'F#7sus2',\n",
       " 'F#7sus4',\n",
       " 'F#m6',\n",
       " 'F#m7',\n",
       " 'F#AmMaj7',\n",
       " 'F#m7b5',\n",
       " 'F#dim6',\n",
       " 'F#M7',\n",
       " 'F#M7+5',\n",
       " 'F#add9',\n",
       " 'F#madd9',\n",
       " 'F#add2',\n",
       " 'F#add4',\n",
       " 'F#add11',\n",
       " 'F#sus4(add13)',\n",
       " 'F#6/9',\n",
       " 'F#6add11',\n",
       " 'F#m6add11',\n",
       " 'F#add4add9',\n",
       " 'F#9',\n",
       " 'F#m9',\n",
       " 'F#M9',\n",
       " 'F#9sus4',\n",
       " 'F#7_6',\n",
       " 'F#7-9',\n",
       " 'F#7+9',\n",
       " 'F#9-5',\n",
       " 'F#9+5',\n",
       " 'F#7#9b5',\n",
       " 'F#7#9#5',\n",
       " 'F#7b9b5',\n",
       " 'F#7b9#5',\n",
       " 'F#11',\n",
       " 'F#7+11',\n",
       " 'F#m7add11',\n",
       " 'F#maj11',\n",
       " 'F#m11',\n",
       " 'F#7b9#9',\n",
       " 'F#7b9#11',\n",
       " 'F#7#9#11',\n",
       " 'F#7-13',\n",
       " 'F#m7add4',\n",
       " 'F#9+11',\n",
       " 'F#m9+11',\n",
       " 'F#13',\n",
       " 'F#maj13',\n",
       " 'F#madd13',\n",
       " 'F#13sus4',\n",
       " 'F#13-9',\n",
       " 'F#13+9',\n",
       " 'F#13+11',\n",
       " 'F#m13+11',\n",
       " 'F#madd11',\n",
       " 'G3',\n",
       " 'G5',\n",
       " 'G',\n",
       " 'Gm',\n",
       " 'Gdim',\n",
       " 'Gaug',\n",
       " 'Gsus2',\n",
       " 'Gsus4',\n",
       " 'G6',\n",
       " 'G7',\n",
       " 'G7-5',\n",
       " 'Gmaj7b5',\n",
       " 'G7+5',\n",
       " 'G6sus2',\n",
       " 'G7sus2',\n",
       " 'G7sus4',\n",
       " 'Gm6',\n",
       " 'Gm7',\n",
       " 'GAmMaj7',\n",
       " 'Gm7b5',\n",
       " 'Gdim6',\n",
       " 'GM7',\n",
       " 'GM7+5',\n",
       " 'Gadd9',\n",
       " 'Gmadd9',\n",
       " 'Gadd2',\n",
       " 'Gadd4',\n",
       " 'Gadd11',\n",
       " 'Gsus4(add13)',\n",
       " 'G6/9',\n",
       " 'G6add11',\n",
       " 'Gm6add11',\n",
       " 'Gadd4add9',\n",
       " 'G9',\n",
       " 'Gm9',\n",
       " 'GM9',\n",
       " 'G9sus4',\n",
       " 'G7_6',\n",
       " 'G7-9',\n",
       " 'G7+9',\n",
       " 'G9-5',\n",
       " 'G9+5',\n",
       " 'G7#9b5',\n",
       " 'G7#9#5',\n",
       " 'G7b9b5',\n",
       " 'G7b9#5',\n",
       " 'G11',\n",
       " 'G7+11',\n",
       " 'Gm7add11',\n",
       " 'Gmaj11',\n",
       " 'Gm11',\n",
       " 'G7b9#9',\n",
       " 'G7b9#11',\n",
       " 'G7#9#11',\n",
       " 'G7-13',\n",
       " 'Gm7add4',\n",
       " 'G9+11',\n",
       " 'Gm9+11',\n",
       " 'G13',\n",
       " 'Gmaj13',\n",
       " 'Gmadd13',\n",
       " 'G13sus4',\n",
       " 'G13-9',\n",
       " 'G13+9',\n",
       " 'G13+11',\n",
       " 'Gm13+11',\n",
       " 'Gmadd11',\n",
       " 'Ab3',\n",
       " 'Ab5',\n",
       " 'Ab',\n",
       " 'Abm',\n",
       " 'Abdim',\n",
       " 'Abaug',\n",
       " 'Absus2',\n",
       " 'Absus4',\n",
       " 'Ab6',\n",
       " 'Ab7',\n",
       " 'Ab7-5',\n",
       " 'Abmaj7b5',\n",
       " 'Ab7+5',\n",
       " 'Ab6sus2',\n",
       " 'Ab7sus2',\n",
       " 'Ab7sus4',\n",
       " 'Abm6',\n",
       " 'Abm7',\n",
       " 'AbAmMaj7',\n",
       " 'Abm7b5',\n",
       " 'Abdim6',\n",
       " 'AbM7',\n",
       " 'AbM7+5',\n",
       " 'Abadd9',\n",
       " 'Abmadd9',\n",
       " 'Abadd2',\n",
       " 'Abadd4',\n",
       " 'Abadd11',\n",
       " 'Absus4(add13)',\n",
       " 'Ab6/9',\n",
       " 'Ab6add11',\n",
       " 'Abm6add11',\n",
       " 'Abadd4add9',\n",
       " 'Ab9',\n",
       " 'Abm9',\n",
       " 'AbM9',\n",
       " 'Ab9sus4',\n",
       " 'Ab7_6',\n",
       " 'Ab7-9',\n",
       " 'Ab7+9',\n",
       " 'Ab9-5',\n",
       " 'Ab9+5',\n",
       " 'Ab7#9b5',\n",
       " 'Ab7#9#5',\n",
       " 'Ab7b9b5',\n",
       " 'Ab7b9#5',\n",
       " 'Ab11',\n",
       " 'Ab7+11',\n",
       " 'Abm7add11',\n",
       " 'Abmaj11',\n",
       " 'Abm11',\n",
       " 'Ab7b9#9',\n",
       " 'Ab7b9#11',\n",
       " 'Ab7#9#11',\n",
       " 'Ab7-13',\n",
       " 'Abm7add4',\n",
       " 'Ab9+11',\n",
       " 'Abm9+11',\n",
       " 'Ab13',\n",
       " 'Abmaj13',\n",
       " 'Abmadd13',\n",
       " 'Ab13sus4',\n",
       " 'Ab13-9',\n",
       " 'Ab13+9',\n",
       " 'Ab13+11',\n",
       " 'Abm13+11',\n",
       " 'Abmadd11',\n",
       " 'A3',\n",
       " 'A5',\n",
       " 'A',\n",
       " 'Am',\n",
       " 'Adim',\n",
       " 'Aaug',\n",
       " 'Asus2',\n",
       " 'Asus4',\n",
       " 'A6',\n",
       " 'A7',\n",
       " 'A7-5',\n",
       " 'Amaj7b5',\n",
       " 'A7+5',\n",
       " 'A6sus2',\n",
       " 'A7sus2',\n",
       " 'A7sus4',\n",
       " 'Am6',\n",
       " 'Am7',\n",
       " 'AAmMaj7',\n",
       " 'Am7b5',\n",
       " 'Adim6',\n",
       " 'AM7',\n",
       " 'AM7+5',\n",
       " 'Aadd9',\n",
       " 'Amadd9',\n",
       " 'Aadd2',\n",
       " 'Aadd4',\n",
       " 'Aadd11',\n",
       " 'Asus4(add13)',\n",
       " 'A6/9',\n",
       " 'A6add11',\n",
       " 'Am6add11',\n",
       " 'Aadd4add9',\n",
       " 'A9',\n",
       " 'Am9',\n",
       " 'AM9',\n",
       " 'A9sus4',\n",
       " 'A7_6',\n",
       " 'A7-9',\n",
       " 'A7+9',\n",
       " 'A9-5',\n",
       " 'A9+5',\n",
       " 'A7#9b5',\n",
       " 'A7#9#5',\n",
       " 'A7b9b5',\n",
       " 'A7b9#5',\n",
       " 'A11',\n",
       " 'A7+11',\n",
       " 'Am7add11',\n",
       " 'Amaj11',\n",
       " 'Am11',\n",
       " 'A7b9#9',\n",
       " 'A7b9#11',\n",
       " 'A7#9#11',\n",
       " 'A7-13',\n",
       " 'Am7add4',\n",
       " 'A9+11',\n",
       " 'Am9+11',\n",
       " 'A13',\n",
       " 'Amaj13',\n",
       " 'Amadd13',\n",
       " 'A13sus4',\n",
       " 'A13-9',\n",
       " 'A13+9',\n",
       " 'A13+11',\n",
       " 'Am13+11',\n",
       " 'Amadd11',\n",
       " 'Bb3',\n",
       " 'Bb5',\n",
       " 'Bb',\n",
       " 'Bbm',\n",
       " 'Bbdim',\n",
       " 'Bbaug',\n",
       " 'Bbsus2',\n",
       " 'Bbsus4',\n",
       " 'Bb6',\n",
       " 'Bb7',\n",
       " 'Bb7-5',\n",
       " 'Bbmaj7b5',\n",
       " 'Bb7+5',\n",
       " 'Bb6sus2',\n",
       " 'Bb7sus2',\n",
       " 'Bb7sus4',\n",
       " 'Bbm6',\n",
       " 'Bbm7',\n",
       " 'BbAmMaj7',\n",
       " 'Bbm7b5',\n",
       " 'Bbdim6',\n",
       " 'BbM7',\n",
       " 'BbM7+5',\n",
       " 'Bbadd9',\n",
       " 'Bbmadd9',\n",
       " 'Bbadd2',\n",
       " 'Bbadd4',\n",
       " 'Bbadd11',\n",
       " 'Bbsus4(add13)',\n",
       " 'Bb6/9',\n",
       " 'Bb6add11',\n",
       " 'Bbm6add11',\n",
       " 'Bbadd4add9',\n",
       " 'Bb9',\n",
       " 'Bbm9',\n",
       " 'BbM9',\n",
       " 'Bb9sus4',\n",
       " 'Bb7_6',\n",
       " 'Bb7-9',\n",
       " 'Bb7+9',\n",
       " 'Bb9-5',\n",
       " 'Bb9+5',\n",
       " 'Bb7#9b5',\n",
       " 'Bb7#9#5',\n",
       " 'Bb7b9b5',\n",
       " 'Bb7b9#5',\n",
       " 'Bb11',\n",
       " 'Bb7+11',\n",
       " 'Bbm7add11',\n",
       " 'Bbmaj11',\n",
       " 'Bbm11',\n",
       " 'Bb7b9#9',\n",
       " 'Bb7b9#11',\n",
       " 'Bb7#9#11',\n",
       " 'Bb7-13',\n",
       " 'Bbm7add4',\n",
       " 'Bb9+11',\n",
       " 'Bbm9+11',\n",
       " 'Bb13',\n",
       " 'Bbmaj13',\n",
       " 'Bbmadd13',\n",
       " 'Bb13sus4',\n",
       " 'Bb13-9',\n",
       " 'Bb13+9',\n",
       " 'Bb13+11',\n",
       " 'Bbm13+11',\n",
       " 'Bbmadd11',\n",
       " 'B3',\n",
       " 'B5',\n",
       " 'B',\n",
       " 'Bm',\n",
       " 'Bdim',\n",
       " 'Baug',\n",
       " 'Bsus2',\n",
       " 'Bsus4',\n",
       " 'B6',\n",
       " 'B7',\n",
       " 'B7-5',\n",
       " 'Bmaj7b5',\n",
       " 'B7+5',\n",
       " 'B6sus2',\n",
       " 'B7sus2',\n",
       " 'B7sus4',\n",
       " 'Bm6',\n",
       " 'Bm7',\n",
       " 'BAmMaj7',\n",
       " 'Bm7b5',\n",
       " 'Bdim6',\n",
       " 'BM7',\n",
       " 'BM7+5',\n",
       " 'Badd9',\n",
       " 'Bmadd9',\n",
       " 'Badd2',\n",
       " 'Badd4',\n",
       " 'Badd11',\n",
       " 'Bsus4(add13)',\n",
       " 'B6/9',\n",
       " 'B6add11',\n",
       " 'Bm6add11',\n",
       " 'Badd4add9',\n",
       " 'B9',\n",
       " 'Bm9',\n",
       " 'BM9',\n",
       " 'B9sus4',\n",
       " 'B7_6',\n",
       " 'B7-9',\n",
       " 'B7+9',\n",
       " 'B9-5',\n",
       " 'B9+5',\n",
       " 'B7#9b5',\n",
       " 'B7#9#5',\n",
       " 'B7b9b5',\n",
       " 'B7b9#5',\n",
       " 'B11',\n",
       " 'B7+11',\n",
       " 'Bm7add11',\n",
       " 'Bmaj11',\n",
       " 'Bm11',\n",
       " 'B7b9#9',\n",
       " 'B7b9#11',\n",
       " 'B7#9#11',\n",
       " 'B7-13',\n",
       " 'Bm7add4',\n",
       " 'B9+11',\n",
       " 'Bm9+11',\n",
       " 'B13',\n",
       " 'Bmaj13',\n",
       " 'Bmadd13',\n",
       " 'B13sus4',\n",
       " 'B13-9',\n",
       " 'B13+9',\n",
       " 'B13+11',\n",
       " 'Bm13+11',\n",
       " 'Bmadd11']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dictionaries.get_all_possible_chords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "804"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(Dictionaries.get_all_possible_chords())\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provides the one_hot() function that creates a hash of each word as an efficient integer encoding. We will estimate the vocabulary size of 100, which is larger than needed to reduce the probability of collisions from the hash function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    def __init__(self,all_chords):\n",
    "        self.chord_to_number_dict = {}\n",
    "        self.number_to_chord_dict = {}\n",
    "        \n",
    "        for number,chord in enumerate(all_chords):\n",
    "            \n",
    "            self.chord_to_number_dict[chord] = number\n",
    "            self.number_to_chord_dict[number] = chord\n",
    "            \n",
    "        print(len(self.chord_to_number_dict))\n",
    "        print(len(self.number_to_chord_dict))\n",
    "\n",
    "    def get_chord_from_number(self, number):\n",
    "        return self.number_to_chord_dict[number]\n",
    "    \n",
    "    def get_number_from_chord(self, chord):\n",
    "        return self.chord_to_number_dict[chord]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "804\n",
      "804\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(Dictionaries.get_all_possible_chords())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'C3',\n",
       " 1: 'C5',\n",
       " 2: 'C',\n",
       " 3: 'Cm',\n",
       " 4: 'Cdim',\n",
       " 5: 'Caug',\n",
       " 6: 'Csus2',\n",
       " 7: 'Csus4',\n",
       " 8: 'C6',\n",
       " 9: 'C7',\n",
       " 10: 'C7-5',\n",
       " 11: 'Cmaj7b5',\n",
       " 12: 'C7+5',\n",
       " 13: 'C6sus2',\n",
       " 14: 'C7sus2',\n",
       " 15: 'C7sus4',\n",
       " 16: 'Cm6',\n",
       " 17: 'Cm7',\n",
       " 18: 'CAmMaj7',\n",
       " 19: 'Cm7b5',\n",
       " 20: 'Cdim6',\n",
       " 21: 'CM7',\n",
       " 22: 'CM7+5',\n",
       " 23: 'Cadd9',\n",
       " 24: 'Cmadd9',\n",
       " 25: 'Cadd2',\n",
       " 26: 'Cadd4',\n",
       " 27: 'Cadd11',\n",
       " 28: 'Csus4(add13)',\n",
       " 29: 'C6/9',\n",
       " 30: 'C6add11',\n",
       " 31: 'Cm6add11',\n",
       " 32: 'Cadd4add9',\n",
       " 33: 'C9',\n",
       " 34: 'Cm9',\n",
       " 35: 'CM9',\n",
       " 36: 'C9sus4',\n",
       " 37: 'C7_6',\n",
       " 38: 'C7-9',\n",
       " 39: 'C7+9',\n",
       " 40: 'C9-5',\n",
       " 41: 'C9+5',\n",
       " 42: 'C7#9b5',\n",
       " 43: 'C7#9#5',\n",
       " 44: 'C7b9b5',\n",
       " 45: 'C7b9#5',\n",
       " 46: 'C11',\n",
       " 47: 'C7+11',\n",
       " 48: 'Cm7add11',\n",
       " 49: 'Cmaj11',\n",
       " 50: 'Cm11',\n",
       " 51: 'C7b9#9',\n",
       " 52: 'C7b9#11',\n",
       " 53: 'C7#9#11',\n",
       " 54: 'C7-13',\n",
       " 55: 'Cm7add4',\n",
       " 56: 'C9+11',\n",
       " 57: 'Cm9+11',\n",
       " 58: 'C13',\n",
       " 59: 'Cmaj13',\n",
       " 60: 'Cmadd13',\n",
       " 61: 'C13sus4',\n",
       " 62: 'C13-9',\n",
       " 63: 'C13+9',\n",
       " 64: 'C13+11',\n",
       " 65: 'Cm13+11',\n",
       " 66: 'Cmadd11',\n",
       " 67: 'Db3',\n",
       " 68: 'Db5',\n",
       " 69: 'Db',\n",
       " 70: 'Dbm',\n",
       " 71: 'Dbdim',\n",
       " 72: 'Dbaug',\n",
       " 73: 'Dbsus2',\n",
       " 74: 'Dbsus4',\n",
       " 75: 'Db6',\n",
       " 76: 'Db7',\n",
       " 77: 'Db7-5',\n",
       " 78: 'Dbmaj7b5',\n",
       " 79: 'Db7+5',\n",
       " 80: 'Db6sus2',\n",
       " 81: 'Db7sus2',\n",
       " 82: 'Db7sus4',\n",
       " 83: 'Dbm6',\n",
       " 84: 'Dbm7',\n",
       " 85: 'DbAmMaj7',\n",
       " 86: 'Dbm7b5',\n",
       " 87: 'Dbdim6',\n",
       " 88: 'DbM7',\n",
       " 89: 'DbM7+5',\n",
       " 90: 'Dbadd9',\n",
       " 91: 'Dbmadd9',\n",
       " 92: 'Dbadd2',\n",
       " 93: 'Dbadd4',\n",
       " 94: 'Dbadd11',\n",
       " 95: 'Dbsus4(add13)',\n",
       " 96: 'Db6/9',\n",
       " 97: 'Db6add11',\n",
       " 98: 'Dbm6add11',\n",
       " 99: 'Dbadd4add9',\n",
       " 100: 'Db9',\n",
       " 101: 'Dbm9',\n",
       " 102: 'DbM9',\n",
       " 103: 'Db9sus4',\n",
       " 104: 'Db7_6',\n",
       " 105: 'Db7-9',\n",
       " 106: 'Db7+9',\n",
       " 107: 'Db9-5',\n",
       " 108: 'Db9+5',\n",
       " 109: 'Db7#9b5',\n",
       " 110: 'Db7#9#5',\n",
       " 111: 'Db7b9b5',\n",
       " 112: 'Db7b9#5',\n",
       " 113: 'Db11',\n",
       " 114: 'Db7+11',\n",
       " 115: 'Dbm7add11',\n",
       " 116: 'Dbmaj11',\n",
       " 117: 'Dbm11',\n",
       " 118: 'Db7b9#9',\n",
       " 119: 'Db7b9#11',\n",
       " 120: 'Db7#9#11',\n",
       " 121: 'Db7-13',\n",
       " 122: 'Dbm7add4',\n",
       " 123: 'Db9+11',\n",
       " 124: 'Dbm9+11',\n",
       " 125: 'Db13',\n",
       " 126: 'Dbmaj13',\n",
       " 127: 'Dbmadd13',\n",
       " 128: 'Db13sus4',\n",
       " 129: 'Db13-9',\n",
       " 130: 'Db13+9',\n",
       " 131: 'Db13+11',\n",
       " 132: 'Dbm13+11',\n",
       " 133: 'Dbmadd11',\n",
       " 134: 'D3',\n",
       " 135: 'D5',\n",
       " 136: 'D',\n",
       " 137: 'Dm',\n",
       " 138: 'Ddim',\n",
       " 139: 'Daug',\n",
       " 140: 'Dsus2',\n",
       " 141: 'Dsus4',\n",
       " 142: 'D6',\n",
       " 143: 'D7',\n",
       " 144: 'D7-5',\n",
       " 145: 'Dmaj7b5',\n",
       " 146: 'D7+5',\n",
       " 147: 'D6sus2',\n",
       " 148: 'D7sus2',\n",
       " 149: 'D7sus4',\n",
       " 150: 'Dm6',\n",
       " 151: 'Dm7',\n",
       " 152: 'DAmMaj7',\n",
       " 153: 'Dm7b5',\n",
       " 154: 'Ddim6',\n",
       " 155: 'DM7',\n",
       " 156: 'DM7+5',\n",
       " 157: 'Dadd9',\n",
       " 158: 'Dmadd9',\n",
       " 159: 'Dadd2',\n",
       " 160: 'Dadd4',\n",
       " 161: 'Dadd11',\n",
       " 162: 'Dsus4(add13)',\n",
       " 163: 'D6/9',\n",
       " 164: 'D6add11',\n",
       " 165: 'Dm6add11',\n",
       " 166: 'Dadd4add9',\n",
       " 167: 'D9',\n",
       " 168: 'Dm9',\n",
       " 169: 'DM9',\n",
       " 170: 'D9sus4',\n",
       " 171: 'D7_6',\n",
       " 172: 'D7-9',\n",
       " 173: 'D7+9',\n",
       " 174: 'D9-5',\n",
       " 175: 'D9+5',\n",
       " 176: 'D7#9b5',\n",
       " 177: 'D7#9#5',\n",
       " 178: 'D7b9b5',\n",
       " 179: 'D7b9#5',\n",
       " 180: 'D11',\n",
       " 181: 'D7+11',\n",
       " 182: 'Dm7add11',\n",
       " 183: 'Dmaj11',\n",
       " 184: 'Dm11',\n",
       " 185: 'D7b9#9',\n",
       " 186: 'D7b9#11',\n",
       " 187: 'D7#9#11',\n",
       " 188: 'D7-13',\n",
       " 189: 'Dm7add4',\n",
       " 190: 'D9+11',\n",
       " 191: 'Dm9+11',\n",
       " 192: 'D13',\n",
       " 193: 'Dmaj13',\n",
       " 194: 'Dmadd13',\n",
       " 195: 'D13sus4',\n",
       " 196: 'D13-9',\n",
       " 197: 'D13+9',\n",
       " 198: 'D13+11',\n",
       " 199: 'Dm13+11',\n",
       " 200: 'Dmadd11',\n",
       " 201: 'Eb3',\n",
       " 202: 'Eb5',\n",
       " 203: 'Eb',\n",
       " 204: 'Ebm',\n",
       " 205: 'Ebdim',\n",
       " 206: 'Ebaug',\n",
       " 207: 'Ebsus2',\n",
       " 208: 'Ebsus4',\n",
       " 209: 'Eb6',\n",
       " 210: 'Eb7',\n",
       " 211: 'Eb7-5',\n",
       " 212: 'Ebmaj7b5',\n",
       " 213: 'Eb7+5',\n",
       " 214: 'Eb6sus2',\n",
       " 215: 'Eb7sus2',\n",
       " 216: 'Eb7sus4',\n",
       " 217: 'Ebm6',\n",
       " 218: 'Ebm7',\n",
       " 219: 'EbAmMaj7',\n",
       " 220: 'Ebm7b5',\n",
       " 221: 'Ebdim6',\n",
       " 222: 'EbM7',\n",
       " 223: 'EbM7+5',\n",
       " 224: 'Ebadd9',\n",
       " 225: 'Ebmadd9',\n",
       " 226: 'Ebadd2',\n",
       " 227: 'Ebadd4',\n",
       " 228: 'Ebadd11',\n",
       " 229: 'Ebsus4(add13)',\n",
       " 230: 'Eb6/9',\n",
       " 231: 'Eb6add11',\n",
       " 232: 'Ebm6add11',\n",
       " 233: 'Ebadd4add9',\n",
       " 234: 'Eb9',\n",
       " 235: 'Ebm9',\n",
       " 236: 'EbM9',\n",
       " 237: 'Eb9sus4',\n",
       " 238: 'Eb7_6',\n",
       " 239: 'Eb7-9',\n",
       " 240: 'Eb7+9',\n",
       " 241: 'Eb9-5',\n",
       " 242: 'Eb9+5',\n",
       " 243: 'Eb7#9b5',\n",
       " 244: 'Eb7#9#5',\n",
       " 245: 'Eb7b9b5',\n",
       " 246: 'Eb7b9#5',\n",
       " 247: 'Eb11',\n",
       " 248: 'Eb7+11',\n",
       " 249: 'Ebm7add11',\n",
       " 250: 'Ebmaj11',\n",
       " 251: 'Ebm11',\n",
       " 252: 'Eb7b9#9',\n",
       " 253: 'Eb7b9#11',\n",
       " 254: 'Eb7#9#11',\n",
       " 255: 'Eb7-13',\n",
       " 256: 'Ebm7add4',\n",
       " 257: 'Eb9+11',\n",
       " 258: 'Ebm9+11',\n",
       " 259: 'Eb13',\n",
       " 260: 'Ebmaj13',\n",
       " 261: 'Ebmadd13',\n",
       " 262: 'Eb13sus4',\n",
       " 263: 'Eb13-9',\n",
       " 264: 'Eb13+9',\n",
       " 265: 'Eb13+11',\n",
       " 266: 'Ebm13+11',\n",
       " 267: 'Ebmadd11',\n",
       " 268: 'E3',\n",
       " 269: 'E5',\n",
       " 270: 'E',\n",
       " 271: 'Em',\n",
       " 272: 'Edim',\n",
       " 273: 'Eaug',\n",
       " 274: 'Esus2',\n",
       " 275: 'Esus4',\n",
       " 276: 'E6',\n",
       " 277: 'E7',\n",
       " 278: 'E7-5',\n",
       " 279: 'Emaj7b5',\n",
       " 280: 'E7+5',\n",
       " 281: 'E6sus2',\n",
       " 282: 'E7sus2',\n",
       " 283: 'E7sus4',\n",
       " 284: 'Em6',\n",
       " 285: 'Em7',\n",
       " 286: 'EAmMaj7',\n",
       " 287: 'Em7b5',\n",
       " 288: 'Edim6',\n",
       " 289: 'EM7',\n",
       " 290: 'EM7+5',\n",
       " 291: 'Eadd9',\n",
       " 292: 'Emadd9',\n",
       " 293: 'Eadd2',\n",
       " 294: 'Eadd4',\n",
       " 295: 'Eadd11',\n",
       " 296: 'Esus4(add13)',\n",
       " 297: 'E6/9',\n",
       " 298: 'E6add11',\n",
       " 299: 'Em6add11',\n",
       " 300: 'Eadd4add9',\n",
       " 301: 'E9',\n",
       " 302: 'Em9',\n",
       " 303: 'EM9',\n",
       " 304: 'E9sus4',\n",
       " 305: 'E7_6',\n",
       " 306: 'E7-9',\n",
       " 307: 'E7+9',\n",
       " 308: 'E9-5',\n",
       " 309: 'E9+5',\n",
       " 310: 'E7#9b5',\n",
       " 311: 'E7#9#5',\n",
       " 312: 'E7b9b5',\n",
       " 313: 'E7b9#5',\n",
       " 314: 'E11',\n",
       " 315: 'E7+11',\n",
       " 316: 'Em7add11',\n",
       " 317: 'Emaj11',\n",
       " 318: 'Em11',\n",
       " 319: 'E7b9#9',\n",
       " 320: 'E7b9#11',\n",
       " 321: 'E7#9#11',\n",
       " 322: 'E7-13',\n",
       " 323: 'Em7add4',\n",
       " 324: 'E9+11',\n",
       " 325: 'Em9+11',\n",
       " 326: 'E13',\n",
       " 327: 'Emaj13',\n",
       " 328: 'Emadd13',\n",
       " 329: 'E13sus4',\n",
       " 330: 'E13-9',\n",
       " 331: 'E13+9',\n",
       " 332: 'E13+11',\n",
       " 333: 'Em13+11',\n",
       " 334: 'Emadd11',\n",
       " 335: 'F3',\n",
       " 336: 'F5',\n",
       " 337: 'F',\n",
       " 338: 'Fm',\n",
       " 339: 'Fdim',\n",
       " 340: 'Faug',\n",
       " 341: 'Fsus2',\n",
       " 342: 'Fsus4',\n",
       " 343: 'F6',\n",
       " 344: 'F7',\n",
       " 345: 'F7-5',\n",
       " 346: 'Fmaj7b5',\n",
       " 347: 'F7+5',\n",
       " 348: 'F6sus2',\n",
       " 349: 'F7sus2',\n",
       " 350: 'F7sus4',\n",
       " 351: 'Fm6',\n",
       " 352: 'Fm7',\n",
       " 353: 'FAmMaj7',\n",
       " 354: 'Fm7b5',\n",
       " 355: 'Fdim6',\n",
       " 356: 'FM7',\n",
       " 357: 'FM7+5',\n",
       " 358: 'Fadd9',\n",
       " 359: 'Fmadd9',\n",
       " 360: 'Fadd2',\n",
       " 361: 'Fadd4',\n",
       " 362: 'Fadd11',\n",
       " 363: 'Fsus4(add13)',\n",
       " 364: 'F6/9',\n",
       " 365: 'F6add11',\n",
       " 366: 'Fm6add11',\n",
       " 367: 'Fadd4add9',\n",
       " 368: 'F9',\n",
       " 369: 'Fm9',\n",
       " 370: 'FM9',\n",
       " 371: 'F9sus4',\n",
       " 372: 'F7_6',\n",
       " 373: 'F7-9',\n",
       " 374: 'F7+9',\n",
       " 375: 'F9-5',\n",
       " 376: 'F9+5',\n",
       " 377: 'F7#9b5',\n",
       " 378: 'F7#9#5',\n",
       " 379: 'F7b9b5',\n",
       " 380: 'F7b9#5',\n",
       " 381: 'F11',\n",
       " 382: 'F7+11',\n",
       " 383: 'Fm7add11',\n",
       " 384: 'Fmaj11',\n",
       " 385: 'Fm11',\n",
       " 386: 'F7b9#9',\n",
       " 387: 'F7b9#11',\n",
       " 388: 'F7#9#11',\n",
       " 389: 'F7-13',\n",
       " 390: 'Fm7add4',\n",
       " 391: 'F9+11',\n",
       " 392: 'Fm9+11',\n",
       " 393: 'F13',\n",
       " 394: 'Fmaj13',\n",
       " 395: 'Fmadd13',\n",
       " 396: 'F13sus4',\n",
       " 397: 'F13-9',\n",
       " 398: 'F13+9',\n",
       " 399: 'F13+11',\n",
       " 400: 'Fm13+11',\n",
       " 401: 'Fmadd11',\n",
       " 402: 'F#3',\n",
       " 403: 'F#5',\n",
       " 404: 'F#',\n",
       " 405: 'F#m',\n",
       " 406: 'F#dim',\n",
       " 407: 'F#aug',\n",
       " 408: 'F#sus2',\n",
       " 409: 'F#sus4',\n",
       " 410: 'F#6',\n",
       " 411: 'F#7',\n",
       " 412: 'F#7-5',\n",
       " 413: 'F#maj7b5',\n",
       " 414: 'F#7+5',\n",
       " 415: 'F#6sus2',\n",
       " 416: 'F#7sus2',\n",
       " 417: 'F#7sus4',\n",
       " 418: 'F#m6',\n",
       " 419: 'F#m7',\n",
       " 420: 'F#AmMaj7',\n",
       " 421: 'F#m7b5',\n",
       " 422: 'F#dim6',\n",
       " 423: 'F#M7',\n",
       " 424: 'F#M7+5',\n",
       " 425: 'F#add9',\n",
       " 426: 'F#madd9',\n",
       " 427: 'F#add2',\n",
       " 428: 'F#add4',\n",
       " 429: 'F#add11',\n",
       " 430: 'F#sus4(add13)',\n",
       " 431: 'F#6/9',\n",
       " 432: 'F#6add11',\n",
       " 433: 'F#m6add11',\n",
       " 434: 'F#add4add9',\n",
       " 435: 'F#9',\n",
       " 436: 'F#m9',\n",
       " 437: 'F#M9',\n",
       " 438: 'F#9sus4',\n",
       " 439: 'F#7_6',\n",
       " 440: 'F#7-9',\n",
       " 441: 'F#7+9',\n",
       " 442: 'F#9-5',\n",
       " 443: 'F#9+5',\n",
       " 444: 'F#7#9b5',\n",
       " 445: 'F#7#9#5',\n",
       " 446: 'F#7b9b5',\n",
       " 447: 'F#7b9#5',\n",
       " 448: 'F#11',\n",
       " 449: 'F#7+11',\n",
       " 450: 'F#m7add11',\n",
       " 451: 'F#maj11',\n",
       " 452: 'F#m11',\n",
       " 453: 'F#7b9#9',\n",
       " 454: 'F#7b9#11',\n",
       " 455: 'F#7#9#11',\n",
       " 456: 'F#7-13',\n",
       " 457: 'F#m7add4',\n",
       " 458: 'F#9+11',\n",
       " 459: 'F#m9+11',\n",
       " 460: 'F#13',\n",
       " 461: 'F#maj13',\n",
       " 462: 'F#madd13',\n",
       " 463: 'F#13sus4',\n",
       " 464: 'F#13-9',\n",
       " 465: 'F#13+9',\n",
       " 466: 'F#13+11',\n",
       " 467: 'F#m13+11',\n",
       " 468: 'F#madd11',\n",
       " 469: 'G3',\n",
       " 470: 'G5',\n",
       " 471: 'G',\n",
       " 472: 'Gm',\n",
       " 473: 'Gdim',\n",
       " 474: 'Gaug',\n",
       " 475: 'Gsus2',\n",
       " 476: 'Gsus4',\n",
       " 477: 'G6',\n",
       " 478: 'G7',\n",
       " 479: 'G7-5',\n",
       " 480: 'Gmaj7b5',\n",
       " 481: 'G7+5',\n",
       " 482: 'G6sus2',\n",
       " 483: 'G7sus2',\n",
       " 484: 'G7sus4',\n",
       " 485: 'Gm6',\n",
       " 486: 'Gm7',\n",
       " 487: 'GAmMaj7',\n",
       " 488: 'Gm7b5',\n",
       " 489: 'Gdim6',\n",
       " 490: 'GM7',\n",
       " 491: 'GM7+5',\n",
       " 492: 'Gadd9',\n",
       " 493: 'Gmadd9',\n",
       " 494: 'Gadd2',\n",
       " 495: 'Gadd4',\n",
       " 496: 'Gadd11',\n",
       " 497: 'Gsus4(add13)',\n",
       " 498: 'G6/9',\n",
       " 499: 'G6add11',\n",
       " 500: 'Gm6add11',\n",
       " 501: 'Gadd4add9',\n",
       " 502: 'G9',\n",
       " 503: 'Gm9',\n",
       " 504: 'GM9',\n",
       " 505: 'G9sus4',\n",
       " 506: 'G7_6',\n",
       " 507: 'G7-9',\n",
       " 508: 'G7+9',\n",
       " 509: 'G9-5',\n",
       " 510: 'G9+5',\n",
       " 511: 'G7#9b5',\n",
       " 512: 'G7#9#5',\n",
       " 513: 'G7b9b5',\n",
       " 514: 'G7b9#5',\n",
       " 515: 'G11',\n",
       " 516: 'G7+11',\n",
       " 517: 'Gm7add11',\n",
       " 518: 'Gmaj11',\n",
       " 519: 'Gm11',\n",
       " 520: 'G7b9#9',\n",
       " 521: 'G7b9#11',\n",
       " 522: 'G7#9#11',\n",
       " 523: 'G7-13',\n",
       " 524: 'Gm7add4',\n",
       " 525: 'G9+11',\n",
       " 526: 'Gm9+11',\n",
       " 527: 'G13',\n",
       " 528: 'Gmaj13',\n",
       " 529: 'Gmadd13',\n",
       " 530: 'G13sus4',\n",
       " 531: 'G13-9',\n",
       " 532: 'G13+9',\n",
       " 533: 'G13+11',\n",
       " 534: 'Gm13+11',\n",
       " 535: 'Gmadd11',\n",
       " 536: 'Ab3',\n",
       " 537: 'Ab5',\n",
       " 538: 'Ab',\n",
       " 539: 'Abm',\n",
       " 540: 'Abdim',\n",
       " 541: 'Abaug',\n",
       " 542: 'Absus2',\n",
       " 543: 'Absus4',\n",
       " 544: 'Ab6',\n",
       " 545: 'Ab7',\n",
       " 546: 'Ab7-5',\n",
       " 547: 'Abmaj7b5',\n",
       " 548: 'Ab7+5',\n",
       " 549: 'Ab6sus2',\n",
       " 550: 'Ab7sus2',\n",
       " 551: 'Ab7sus4',\n",
       " 552: 'Abm6',\n",
       " 553: 'Abm7',\n",
       " 554: 'AbAmMaj7',\n",
       " 555: 'Abm7b5',\n",
       " 556: 'Abdim6',\n",
       " 557: 'AbM7',\n",
       " 558: 'AbM7+5',\n",
       " 559: 'Abadd9',\n",
       " 560: 'Abmadd9',\n",
       " 561: 'Abadd2',\n",
       " 562: 'Abadd4',\n",
       " 563: 'Abadd11',\n",
       " 564: 'Absus4(add13)',\n",
       " 565: 'Ab6/9',\n",
       " 566: 'Ab6add11',\n",
       " 567: 'Abm6add11',\n",
       " 568: 'Abadd4add9',\n",
       " 569: 'Ab9',\n",
       " 570: 'Abm9',\n",
       " 571: 'AbM9',\n",
       " 572: 'Ab9sus4',\n",
       " 573: 'Ab7_6',\n",
       " 574: 'Ab7-9',\n",
       " 575: 'Ab7+9',\n",
       " 576: 'Ab9-5',\n",
       " 577: 'Ab9+5',\n",
       " 578: 'Ab7#9b5',\n",
       " 579: 'Ab7#9#5',\n",
       " 580: 'Ab7b9b5',\n",
       " 581: 'Ab7b9#5',\n",
       " 582: 'Ab11',\n",
       " 583: 'Ab7+11',\n",
       " 584: 'Abm7add11',\n",
       " 585: 'Abmaj11',\n",
       " 586: 'Abm11',\n",
       " 587: 'Ab7b9#9',\n",
       " 588: 'Ab7b9#11',\n",
       " 589: 'Ab7#9#11',\n",
       " 590: 'Ab7-13',\n",
       " 591: 'Abm7add4',\n",
       " 592: 'Ab9+11',\n",
       " 593: 'Abm9+11',\n",
       " 594: 'Ab13',\n",
       " 595: 'Abmaj13',\n",
       " 596: 'Abmadd13',\n",
       " 597: 'Ab13sus4',\n",
       " 598: 'Ab13-9',\n",
       " 599: 'Ab13+9',\n",
       " 600: 'Ab13+11',\n",
       " 601: 'Abm13+11',\n",
       " 602: 'Abmadd11',\n",
       " 603: 'A3',\n",
       " 604: 'A5',\n",
       " 605: 'A',\n",
       " 606: 'Am',\n",
       " 607: 'Adim',\n",
       " 608: 'Aaug',\n",
       " 609: 'Asus2',\n",
       " 610: 'Asus4',\n",
       " 611: 'A6',\n",
       " 612: 'A7',\n",
       " 613: 'A7-5',\n",
       " 614: 'Amaj7b5',\n",
       " 615: 'A7+5',\n",
       " 616: 'A6sus2',\n",
       " 617: 'A7sus2',\n",
       " 618: 'A7sus4',\n",
       " 619: 'Am6',\n",
       " 620: 'Am7',\n",
       " 621: 'AAmMaj7',\n",
       " 622: 'Am7b5',\n",
       " 623: 'Adim6',\n",
       " 624: 'AM7',\n",
       " 625: 'AM7+5',\n",
       " 626: 'Aadd9',\n",
       " 627: 'Amadd9',\n",
       " 628: 'Aadd2',\n",
       " 629: 'Aadd4',\n",
       " 630: 'Aadd11',\n",
       " 631: 'Asus4(add13)',\n",
       " 632: 'A6/9',\n",
       " 633: 'A6add11',\n",
       " 634: 'Am6add11',\n",
       " 635: 'Aadd4add9',\n",
       " 636: 'A9',\n",
       " 637: 'Am9',\n",
       " 638: 'AM9',\n",
       " 639: 'A9sus4',\n",
       " 640: 'A7_6',\n",
       " 641: 'A7-9',\n",
       " 642: 'A7+9',\n",
       " 643: 'A9-5',\n",
       " 644: 'A9+5',\n",
       " 645: 'A7#9b5',\n",
       " 646: 'A7#9#5',\n",
       " 647: 'A7b9b5',\n",
       " 648: 'A7b9#5',\n",
       " 649: 'A11',\n",
       " 650: 'A7+11',\n",
       " 651: 'Am7add11',\n",
       " 652: 'Amaj11',\n",
       " 653: 'Am11',\n",
       " 654: 'A7b9#9',\n",
       " 655: 'A7b9#11',\n",
       " 656: 'A7#9#11',\n",
       " 657: 'A7-13',\n",
       " 658: 'Am7add4',\n",
       " 659: 'A9+11',\n",
       " 660: 'Am9+11',\n",
       " 661: 'A13',\n",
       " 662: 'Amaj13',\n",
       " 663: 'Amadd13',\n",
       " 664: 'A13sus4',\n",
       " 665: 'A13-9',\n",
       " 666: 'A13+9',\n",
       " 667: 'A13+11',\n",
       " 668: 'Am13+11',\n",
       " 669: 'Amadd11',\n",
       " 670: 'Bb3',\n",
       " 671: 'Bb5',\n",
       " 672: 'Bb',\n",
       " 673: 'Bbm',\n",
       " 674: 'Bbdim',\n",
       " 675: 'Bbaug',\n",
       " 676: 'Bbsus2',\n",
       " 677: 'Bbsus4',\n",
       " 678: 'Bb6',\n",
       " 679: 'Bb7',\n",
       " 680: 'Bb7-5',\n",
       " 681: 'Bbmaj7b5',\n",
       " 682: 'Bb7+5',\n",
       " 683: 'Bb6sus2',\n",
       " 684: 'Bb7sus2',\n",
       " 685: 'Bb7sus4',\n",
       " 686: 'Bbm6',\n",
       " 687: 'Bbm7',\n",
       " 688: 'BbAmMaj7',\n",
       " 689: 'Bbm7b5',\n",
       " 690: 'Bbdim6',\n",
       " 691: 'BbM7',\n",
       " 692: 'BbM7+5',\n",
       " 693: 'Bbadd9',\n",
       " 694: 'Bbmadd9',\n",
       " 695: 'Bbadd2',\n",
       " 696: 'Bbadd4',\n",
       " 697: 'Bbadd11',\n",
       " 698: 'Bbsus4(add13)',\n",
       " 699: 'Bb6/9',\n",
       " 700: 'Bb6add11',\n",
       " 701: 'Bbm6add11',\n",
       " 702: 'Bbadd4add9',\n",
       " 703: 'Bb9',\n",
       " 704: 'Bbm9',\n",
       " 705: 'BbM9',\n",
       " 706: 'Bb9sus4',\n",
       " 707: 'Bb7_6',\n",
       " 708: 'Bb7-9',\n",
       " 709: 'Bb7+9',\n",
       " 710: 'Bb9-5',\n",
       " 711: 'Bb9+5',\n",
       " 712: 'Bb7#9b5',\n",
       " 713: 'Bb7#9#5',\n",
       " 714: 'Bb7b9b5',\n",
       " 715: 'Bb7b9#5',\n",
       " 716: 'Bb11',\n",
       " 717: 'Bb7+11',\n",
       " 718: 'Bbm7add11',\n",
       " 719: 'Bbmaj11',\n",
       " 720: 'Bbm11',\n",
       " 721: 'Bb7b9#9',\n",
       " 722: 'Bb7b9#11',\n",
       " 723: 'Bb7#9#11',\n",
       " 724: 'Bb7-13',\n",
       " 725: 'Bbm7add4',\n",
       " 726: 'Bb9+11',\n",
       " 727: 'Bbm9+11',\n",
       " 728: 'Bb13',\n",
       " 729: 'Bbmaj13',\n",
       " 730: 'Bbmadd13',\n",
       " 731: 'Bb13sus4',\n",
       " 732: 'Bb13-9',\n",
       " 733: 'Bb13+9',\n",
       " 734: 'Bb13+11',\n",
       " 735: 'Bbm13+11',\n",
       " 736: 'Bbmadd11',\n",
       " 737: 'B3',\n",
       " 738: 'B5',\n",
       " 739: 'B',\n",
       " 740: 'Bm',\n",
       " 741: 'Bdim',\n",
       " 742: 'Baug',\n",
       " 743: 'Bsus2',\n",
       " 744: 'Bsus4',\n",
       " 745: 'B6',\n",
       " 746: 'B7',\n",
       " 747: 'B7-5',\n",
       " 748: 'Bmaj7b5',\n",
       " 749: 'B7+5',\n",
       " 750: 'B6sus2',\n",
       " 751: 'B7sus2',\n",
       " 752: 'B7sus4',\n",
       " 753: 'Bm6',\n",
       " 754: 'Bm7',\n",
       " 755: 'BAmMaj7',\n",
       " 756: 'Bm7b5',\n",
       " 757: 'Bdim6',\n",
       " 758: 'BM7',\n",
       " 759: 'BM7+5',\n",
       " 760: 'Badd9',\n",
       " 761: 'Bmadd9',\n",
       " 762: 'Badd2',\n",
       " 763: 'Badd4',\n",
       " 764: 'Badd11',\n",
       " 765: 'Bsus4(add13)',\n",
       " 766: 'B6/9',\n",
       " 767: 'B6add11',\n",
       " 768: 'Bm6add11',\n",
       " 769: 'Badd4add9',\n",
       " 770: 'B9',\n",
       " 771: 'Bm9',\n",
       " 772: 'BM9',\n",
       " 773: 'B9sus4',\n",
       " 774: 'B7_6',\n",
       " 775: 'B7-9',\n",
       " 776: 'B7+9',\n",
       " 777: 'B9-5',\n",
       " 778: 'B9+5',\n",
       " 779: 'B7#9b5',\n",
       " 780: 'B7#9#5',\n",
       " 781: 'B7b9b5',\n",
       " 782: 'B7b9#5',\n",
       " 783: 'B11',\n",
       " 784: 'B7+11',\n",
       " 785: 'Bm7add11',\n",
       " 786: 'Bmaj11',\n",
       " 787: 'Bm11',\n",
       " 788: 'B7b9#9',\n",
       " 789: 'B7b9#11',\n",
       " 790: 'B7#9#11',\n",
       " 791: 'B7-13',\n",
       " 792: 'Bm7add4',\n",
       " 793: 'B9+11',\n",
       " 794: 'Bm9+11',\n",
       " 795: 'B13',\n",
       " 796: 'Bmaj13',\n",
       " 797: 'Bmadd13',\n",
       " 798: 'B13sus4',\n",
       " 799: 'B13-9',\n",
       " 800: 'B13+9',\n",
       " 801: 'B13+11',\n",
       " 802: 'Bm13+11',\n",
       " 803: 'Bmadd11'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.number_to_chord_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jl_song_data import SongData\n",
    "from jl_constants import NUMBER_OF_NOTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = SongData('../data/chords_clean_1_2_3_4_5_6.csv')\n",
    "df = data.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['chords'] = df['chords'].apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jl_chord_parser import ChordParser\n",
    "parse = ChordParser().parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['chord_numbers'] = df['chords']\\\n",
    "        .apply(lambda chords: [parse(chord) for chord in chords])\\\n",
    "        .apply(lambda chords: [encoder.get_number_from_chord(chord.standard_name) for chord in chords if chord is not None ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>name</th>\n",
       "      <th>decade</th>\n",
       "      <th>genre</th>\n",
       "      <th>chords</th>\n",
       "      <th>uuid</th>\n",
       "      <th>chord_numbers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11754</td>\n",
       "      <td>https://tabs.ultimate-guitar.com/tab/tame_impa...</td>\n",
       "      <td>The Less I Know The Better (ver 2)</td>\n",
       "      <td>2010s</td>\n",
       "      <td>Rock%%Electronic</td>\n",
       "      <td>[Dbm, B, E, Abm, B, Dbm, B, E, Abm, Dbm, B, E,...</td>\n",
       "      <td>22339020-017e-4b6b-ab69-688d9450bb2a</td>\n",
       "      <td>[70, 739, 270, 539, 739, 70, 739, 270, 539, 70...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8406</td>\n",
       "      <td>https://tabs.ultimate-guitar.com/tab/melanie-c...</td>\n",
       "      <td>I Want Candy</td>\n",
       "      <td>2000s</td>\n",
       "      <td>Pop</td>\n",
       "      <td>[C, C, C, F, C, C, C, F, C, C, C, F, C, C, C, ...</td>\n",
       "      <td>8222fda1-c030-49a6-b916-490afc0284ae</td>\n",
       "      <td>[2, 2, 2, 337, 2, 2, 2, 337, 2, 2, 2, 337, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9940</td>\n",
       "      <td>https://tabs.ultimate-guitar.com/tab/pointer-s...</td>\n",
       "      <td>Im So Excited (ver 3)</td>\n",
       "      <td>1980s</td>\n",
       "      <td>Disco%%Disco%%Rhythm And Blues</td>\n",
       "      <td>[Gm7, Bb/C, Cm7, Bb/C, Cm7, Bb/C, Cm7, Bb/Eb, ...</td>\n",
       "      <td>9bf7b788-df6c-46f8-90a0-5d6c7139796a</td>\n",
       "      <td>[486, 672, 17, 672, 17, 672, 17, 672, 203, 672...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4595</td>\n",
       "      <td>https://tabs.ultimate-guitar.com/tab/frank-sin...</td>\n",
       "      <td>Be Careful Its My Heart (ver 2)</td>\n",
       "      <td>1960s</td>\n",
       "      <td>Jazz</td>\n",
       "      <td>[F6, A7, Bb, F, Am7, D7, G, C7+, C7, Gm7, C7, ...</td>\n",
       "      <td>8cf41429-5214-434f-9dd9-623bacafa43e</td>\n",
       "      <td>[343, 612, 672, 337, 620, 143, 471, 12, 9, 486...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>https://tabs.ultimate-guitar.com/tab/1136507</td>\n",
       "      <td>Clown</td>\n",
       "      <td>2010s</td>\n",
       "      <td>Rhythm And Blues%%Contemporary R&amp;b</td>\n",
       "      <td>[Am, D/F#, G, Am, D/F#, G, Am, D/F#, Em, C, Cm...</td>\n",
       "      <td>847a9a07-9773-4eab-a5c6-6e0cbe7f7357</td>\n",
       "      <td>[606, 136, 471, 606, 136, 471, 606, 136, 271, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5548</td>\n",
       "      <td>https://tabs.ultimate-guitar.com/tab/hillsong-...</td>\n",
       "      <td>Love Goes On (ver 4)</td>\n",
       "      <td>2010s</td>\n",
       "      <td>Religious Music</td>\n",
       "      <td>[A, C#m, E, A, C#m, E, B, A, C#m, E, B, A, C#m...</td>\n",
       "      <td>3a70c6ee-5e0c-4d0c-a5d2-ab2963c54974</td>\n",
       "      <td>[605, 70, 270, 605, 70, 270, 739, 605, 70, 270...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12798</td>\n",
       "      <td>https://tabs.ultimate-guitar.com/tab/the_verve...</td>\n",
       "      <td>Bitter Sweet Symphony (ver 2)</td>\n",
       "      <td>1990s</td>\n",
       "      <td>Rock</td>\n",
       "      <td>[E, Bm7, Asus4, A, E, E, Bm7, Asus4, A, E, Bm7...</td>\n",
       "      <td>db01536b-0ed1-4f45-8be9-d4127bba2b21</td>\n",
       "      <td>[270, 754, 610, 605, 270, 270, 754, 610, 605, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12331</td>\n",
       "      <td>https://tabs.ultimate-guitar.com/tab/the_everl...</td>\n",
       "      <td>Long Time Gone (ver 2)</td>\n",
       "      <td>1950s</td>\n",
       "      <td>Folk</td>\n",
       "      <td>[D, A7, G, D, A7, D, G, D, A7, D, D, A7, G, D,...</td>\n",
       "      <td>b59ac840-9cd2-472a-84cc-49b866ffdbd2</td>\n",
       "      <td>[136, 612, 471, 136, 612, 136, 471, 136, 612, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13816</td>\n",
       "      <td>https://tabs.ultimate-guitar.com/tab/whigfield...</td>\n",
       "      <td>Close To You (ver 2)</td>\n",
       "      <td>1990s</td>\n",
       "      <td>Electronic</td>\n",
       "      <td>[D, F#m, G, Bm, A, Gm, Em, D, G, A, D, G, A, D...</td>\n",
       "      <td>b6021f85-3235-4268-91a2-28c143f3a701</td>\n",
       "      <td>[136, 405, 471, 740, 605, 472, 271, 136, 471, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9893</td>\n",
       "      <td>https://tabs.ultimate-guitar.com/tab/pink-floy...</td>\n",
       "      <td>Hey You</td>\n",
       "      <td>1970s</td>\n",
       "      <td>Rock</td>\n",
       "      <td>[Em, Dm, Em, Dm, Emadd9, Bm, Emadd9, Bm, D, D7...</td>\n",
       "      <td>68bfbf09-aa16-473f-b928-ff58c0ae0075</td>\n",
       "      <td>[271, 137, 271, 137, 292, 740, 292, 740, 136, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     url  \\\n",
       "11754  https://tabs.ultimate-guitar.com/tab/tame_impa...   \n",
       "8406   https://tabs.ultimate-guitar.com/tab/melanie-c...   \n",
       "9940   https://tabs.ultimate-guitar.com/tab/pointer-s...   \n",
       "4595   https://tabs.ultimate-guitar.com/tab/frank-sin...   \n",
       "13          https://tabs.ultimate-guitar.com/tab/1136507   \n",
       "5548   https://tabs.ultimate-guitar.com/tab/hillsong-...   \n",
       "12798  https://tabs.ultimate-guitar.com/tab/the_verve...   \n",
       "12331  https://tabs.ultimate-guitar.com/tab/the_everl...   \n",
       "13816  https://tabs.ultimate-guitar.com/tab/whigfield...   \n",
       "9893   https://tabs.ultimate-guitar.com/tab/pink-floy...   \n",
       "\n",
       "                                     name decade  \\\n",
       "11754  The Less I Know The Better (ver 2)  2010s   \n",
       "8406                         I Want Candy  2000s   \n",
       "9940                Im So Excited (ver 3)  1980s   \n",
       "4595      Be Careful Its My Heart (ver 2)  1960s   \n",
       "13                                  Clown  2010s   \n",
       "5548                 Love Goes On (ver 4)  2010s   \n",
       "12798       Bitter Sweet Symphony (ver 2)  1990s   \n",
       "12331              Long Time Gone (ver 2)  1950s   \n",
       "13816                Close To You (ver 2)  1990s   \n",
       "9893                              Hey You  1970s   \n",
       "\n",
       "                                    genre  \\\n",
       "11754                    Rock%%Electronic   \n",
       "8406                                  Pop   \n",
       "9940       Disco%%Disco%%Rhythm And Blues   \n",
       "4595                                 Jazz   \n",
       "13     Rhythm And Blues%%Contemporary R&b   \n",
       "5548                      Religious Music   \n",
       "12798                                Rock   \n",
       "12331                                Folk   \n",
       "13816                          Electronic   \n",
       "9893                                 Rock   \n",
       "\n",
       "                                                  chords  \\\n",
       "11754  [Dbm, B, E, Abm, B, Dbm, B, E, Abm, Dbm, B, E,...   \n",
       "8406   [C, C, C, F, C, C, C, F, C, C, C, F, C, C, C, ...   \n",
       "9940   [Gm7, Bb/C, Cm7, Bb/C, Cm7, Bb/C, Cm7, Bb/Eb, ...   \n",
       "4595   [F6, A7, Bb, F, Am7, D7, G, C7+, C7, Gm7, C7, ...   \n",
       "13     [Am, D/F#, G, Am, D/F#, G, Am, D/F#, Em, C, Cm...   \n",
       "5548   [A, C#m, E, A, C#m, E, B, A, C#m, E, B, A, C#m...   \n",
       "12798  [E, Bm7, Asus4, A, E, E, Bm7, Asus4, A, E, Bm7...   \n",
       "12331  [D, A7, G, D, A7, D, G, D, A7, D, D, A7, G, D,...   \n",
       "13816  [D, F#m, G, Bm, A, Gm, Em, D, G, A, D, G, A, D...   \n",
       "9893   [Em, Dm, Em, Dm, Emadd9, Bm, Emadd9, Bm, D, D7...   \n",
       "\n",
       "                                       uuid  \\\n",
       "11754  22339020-017e-4b6b-ab69-688d9450bb2a   \n",
       "8406   8222fda1-c030-49a6-b916-490afc0284ae   \n",
       "9940   9bf7b788-df6c-46f8-90a0-5d6c7139796a   \n",
       "4595   8cf41429-5214-434f-9dd9-623bacafa43e   \n",
       "13     847a9a07-9773-4eab-a5c6-6e0cbe7f7357   \n",
       "5548   3a70c6ee-5e0c-4d0c-a5d2-ab2963c54974   \n",
       "12798  db01536b-0ed1-4f45-8be9-d4127bba2b21   \n",
       "12331  b59ac840-9cd2-472a-84cc-49b866ffdbd2   \n",
       "13816  b6021f85-3235-4268-91a2-28c143f3a701   \n",
       "9893   68bfbf09-aa16-473f-b928-ff58c0ae0075   \n",
       "\n",
       "                                           chord_numbers  \n",
       "11754  [70, 739, 270, 539, 739, 70, 739, 270, 539, 70...  \n",
       "8406   [2, 2, 2, 337, 2, 2, 2, 337, 2, 2, 2, 337, 2, ...  \n",
       "9940   [486, 672, 17, 672, 17, 672, 17, 672, 203, 672...  \n",
       "4595   [343, 612, 672, 337, 620, 143, 471, 12, 9, 486...  \n",
       "13     [606, 136, 471, 606, 136, 471, 606, 136, 271, ...  \n",
       "5548   [605, 70, 270, 605, 70, 270, 739, 605, 70, 270...  \n",
       "12798  [270, 754, 610, 605, 270, 270, 754, 610, 605, ...  \n",
       "12331  [136, 612, 471, 136, 612, 136, 471, 136, 612, ...  \n",
       "13816  [136, 405, 471, 740, 605, 472, 271, 136, 471, ...  \n",
       "9893   [271, 137, 271, 137, 292, 740, 292, 740, 136, ...  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['chord_numbers'].apply(lambda chords: len(chords) > 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "url              https://tabs.ultimate-guitar.com/tab/10000_man...\n",
       "name                                                     Dont Talk\n",
       "decade                                                       1980s\n",
       "genre                                                         Folk\n",
       "chords           [D, Dmaj7, D, Dmaj7, D, Dmaj7, D, Dmaj7, D, Dm...\n",
       "uuid                          c639eb23-fefd-4263-af20-3f78f110edcd\n",
       "chord_numbers    [136, 155, 136, 155, 136, 155, 136, 155, 136, ...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 5\n",
    "step = 1\n",
    "sentences = []\n",
    "next_chords = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    sequence = row['chord_numbers']\n",
    "    for i in range(0, len(sequence) - maxlen, step):\n",
    "        sentences.append(sequence[i: i + maxlen])\n",
    "        next_chords.append(sequence[i + maxlen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1136924"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1136924"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(next_chords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[136, 155, 136, 155, 136]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_chords[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_chords = to_categorical(next_chords, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "804"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(next_chords[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array(next_chords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 5, 5)              4020      \n",
      "_________________________________________________________________\n",
      "lstm_17 (LSTM)               (None, 5, 50)             11200     \n",
      "_________________________________________________________________\n",
      "lstm_18 (LSTM)               (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 804)               41004     \n",
      "=================================================================\n",
      "Total params: 78,974\n",
      "Trainable params: 78,974\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "# https://medium.com/towards-artificial-intelligence/sentence-prediction-using-word-level-lstm-text-generator-language-modeling-using-rnn-a80c4cda5b40\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, maxlen,input_length=maxlen))\n",
    "model.add(LSTM(50,return_sequences=True))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(50,activation='relu'))\n",
    "model.add(Dense(vocab_size,activation='softmax'))\n",
    "opt_adam = optimizers.adam(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=opt_adam,metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "1136924/1136924 [==============================] - 94s 83us/step - loss: 2.6008 - accuracy: 0.2543\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.60075, saving model to word_pred_Model4.h5\n",
      "Epoch 2/500\n",
      "1136924/1136924 [==============================] - 92s 81us/step - loss: 2.2008 - accuracy: 0.3976\n",
      "\n",
      "Epoch 00002: loss improved from 2.60075 to 2.20085, saving model to word_pred_Model4.h5\n",
      "Epoch 3/500\n",
      "1136924/1136924 [==============================] - 93s 82us/step - loss: 2.0702 - accuracy: 0.4405\n",
      "\n",
      "Epoch 00003: loss improved from 2.20085 to 2.07017, saving model to word_pred_Model4.h5\n",
      "Epoch 4/500\n",
      "1136924/1136924 [==============================] - 96s 84us/step - loss: 2.0029 - accuracy: 0.4599\n",
      "\n",
      "Epoch 00004: loss improved from 2.07017 to 2.00286, saving model to word_pred_Model4.h5\n",
      "Epoch 5/500\n",
      "1136924/1136924 [==============================] - 94s 83us/step - loss: 1.9609 - accuracy: 0.4715\n",
      "\n",
      "Epoch 00005: loss improved from 2.00286 to 1.96089, saving model to word_pred_Model4.h5\n",
      "Epoch 6/500\n",
      "1136924/1136924 [==============================] - 86s 76us/step - loss: 1.9320 - accuracy: 0.4798\n",
      "\n",
      "Epoch 00006: loss improved from 1.96089 to 1.93198, saving model to word_pred_Model4.h5\n",
      "Epoch 7/500\n",
      "1136924/1136924 [==============================] - 90s 79us/step - loss: 1.9099 - accuracy: 0.4856\n",
      "\n",
      "Epoch 00007: loss improved from 1.93198 to 1.90992, saving model to word_pred_Model4.h5\n",
      "Epoch 8/500\n",
      "1136924/1136924 [==============================] - 90s 79us/step - loss: 1.8931 - accuracy: 0.4904\n",
      "\n",
      "Epoch 00008: loss improved from 1.90992 to 1.89309, saving model to word_pred_Model4.h5\n",
      "Epoch 9/500\n",
      "1136924/1136924 [==============================] - 90s 79us/step - loss: 1.8793 - accuracy: 0.4943\n",
      "\n",
      "Epoch 00009: loss improved from 1.89309 to 1.87933, saving model to word_pred_Model4.h5\n",
      "Epoch 10/500\n",
      "1136924/1136924 [==============================] - 90s 79us/step - loss: 1.8682 - accuracy: 0.4978\n",
      "\n",
      "Epoch 00010: loss improved from 1.87933 to 1.86821, saving model to word_pred_Model4.h5\n",
      "Epoch 11/500\n",
      "1136924/1136924 [==============================] - 90s 79us/step - loss: 1.8587 - accuracy: 0.5001\n",
      "\n",
      "Epoch 00011: loss improved from 1.86821 to 1.85874, saving model to word_pred_Model4.h5\n",
      "Epoch 12/500\n",
      "1136924/1136924 [==============================] - 90s 79us/step - loss: 1.8505 - accuracy: 0.5019\n",
      "\n",
      "Epoch 00012: loss improved from 1.85874 to 1.85055, saving model to word_pred_Model4.h5\n",
      "Epoch 13/500\n",
      "1136924/1136924 [==============================] - 90s 79us/step - loss: 1.8436 - accuracy: 0.5041\n",
      "\n",
      "Epoch 00013: loss improved from 1.85055 to 1.84363, saving model to word_pred_Model4.h5\n",
      "Epoch 14/500\n",
      "1136924/1136924 [==============================] - 81s 71us/step - loss: 1.8377 - accuracy: 0.5055\n",
      "\n",
      "Epoch 00014: loss improved from 1.84363 to 1.83772, saving model to word_pred_Model4.h5\n",
      "Epoch 15/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.8322 - accuracy: 0.5071\n",
      "\n",
      "Epoch 00015: loss improved from 1.83772 to 1.83220, saving model to word_pred_Model4.h5\n",
      "Epoch 16/500\n",
      "1136924/1136924 [==============================] - 89s 78us/step - loss: 1.8271 - accuracy: 0.5084\n",
      "\n",
      "Epoch 00016: loss improved from 1.83220 to 1.82714, saving model to word_pred_Model4.h5\n",
      "Epoch 17/500\n",
      "1136924/1136924 [==============================] - 90s 80us/step - loss: 1.8227 - accuracy: 0.5094\n",
      "\n",
      "Epoch 00017: loss improved from 1.82714 to 1.82274, saving model to word_pred_Model4.h5\n",
      "Epoch 18/500\n",
      "1136924/1136924 [==============================] - 90s 79us/step - loss: 1.8190 - accuracy: 0.5109\n",
      "\n",
      "Epoch 00018: loss improved from 1.82274 to 1.81897, saving model to word_pred_Model4.h5\n",
      "Epoch 19/500\n",
      "1136924/1136924 [==============================] - 90s 79us/step - loss: 1.8152 - accuracy: 0.5117\n",
      "\n",
      "Epoch 00019: loss improved from 1.81897 to 1.81521, saving model to word_pred_Model4.h5\n",
      "Epoch 20/500\n",
      "1136924/1136924 [==============================] - 90s 79us/step - loss: 1.8117 - accuracy: 0.5126\n",
      "\n",
      "Epoch 00020: loss improved from 1.81521 to 1.81169, saving model to word_pred_Model4.h5\n",
      "Epoch 21/500\n",
      "1136924/1136924 [==============================] - 90s 79us/step - loss: 1.8087 - accuracy: 0.5135\n",
      "\n",
      "Epoch 00021: loss improved from 1.81169 to 1.80866, saving model to word_pred_Model4.h5\n",
      "Epoch 22/500\n",
      "1136924/1136924 [==============================] - 90s 79us/step - loss: 1.8060 - accuracy: 0.5141\n",
      "\n",
      "Epoch 00022: loss improved from 1.80866 to 1.80595, saving model to word_pred_Model4.h5\n",
      "Epoch 23/500\n",
      "1136924/1136924 [==============================] - 90s 79us/step - loss: 1.8031 - accuracy: 0.5150\n",
      "\n",
      "Epoch 00023: loss improved from 1.80595 to 1.80310, saving model to word_pred_Model4.h5\n",
      "Epoch 24/500\n",
      "1136924/1136924 [==============================] - 90s 79us/step - loss: 1.8005 - accuracy: 0.5156\n",
      "\n",
      "Epoch 00024: loss improved from 1.80310 to 1.80054, saving model to word_pred_Model4.h5\n",
      "Epoch 25/500\n",
      "1136924/1136924 [==============================] - 90s 79us/step - loss: 1.7983 - accuracy: 0.5158\n",
      "\n",
      "Epoch 00025: loss improved from 1.80054 to 1.79825, saving model to word_pred_Model4.h5\n",
      "Epoch 26/500\n",
      "1136924/1136924 [==============================] - 90s 79us/step - loss: 1.7962 - accuracy: 0.5165\n",
      "\n",
      "Epoch 00026: loss improved from 1.79825 to 1.79620, saving model to word_pred_Model4.h5\n",
      "Epoch 27/500\n",
      "1136924/1136924 [==============================] - 90s 79us/step - loss: 1.7940 - accuracy: 0.5171\n",
      "\n",
      "Epoch 00027: loss improved from 1.79620 to 1.79399, saving model to word_pred_Model4.h5\n",
      "Epoch 28/500\n",
      "1136924/1136924 [==============================] - 84s 74us/step - loss: 1.7921 - accuracy: 0.5177\n",
      "\n",
      "Epoch 00028: loss improved from 1.79399 to 1.79215, saving model to word_pred_Model4.h5\n",
      "Epoch 29/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7903 - accuracy: 0.5183\n",
      "\n",
      "Epoch 00029: loss improved from 1.79215 to 1.79035, saving model to word_pred_Model4.h5\n",
      "Epoch 30/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7889 - accuracy: 0.5188\n",
      "\n",
      "Epoch 00030: loss improved from 1.79035 to 1.78890, saving model to word_pred_Model4.h5\n",
      "Epoch 31/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7870 - accuracy: 0.5188\n",
      "\n",
      "Epoch 00031: loss improved from 1.78890 to 1.78700, saving model to word_pred_Model4.h5\n",
      "Epoch 32/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7859 - accuracy: 0.5192\n",
      "\n",
      "Epoch 00032: loss improved from 1.78700 to 1.78591, saving model to word_pred_Model4.h5\n",
      "Epoch 33/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7845 - accuracy: 0.5200\n",
      "\n",
      "Epoch 00033: loss improved from 1.78591 to 1.78452, saving model to word_pred_Model4.h5\n",
      "Epoch 34/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7831 - accuracy: 0.5201\n",
      "\n",
      "Epoch 00034: loss improved from 1.78452 to 1.78312, saving model to word_pred_Model4.h5\n",
      "Epoch 35/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7820 - accuracy: 0.5203\n",
      "\n",
      "Epoch 00035: loss improved from 1.78312 to 1.78197, saving model to word_pred_Model4.h5\n",
      "Epoch 36/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7808 - accuracy: 0.5208\n",
      "\n",
      "Epoch 00036: loss improved from 1.78197 to 1.78076, saving model to word_pred_Model4.h5\n",
      "Epoch 37/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7797 - accuracy: 0.5209\n",
      "\n",
      "Epoch 00037: loss improved from 1.78076 to 1.77966, saving model to word_pred_Model4.h5\n",
      "Epoch 38/500\n",
      "1136924/1136924 [==============================] - 81s 71us/step - loss: 1.7785 - accuracy: 0.5215\n",
      "\n",
      "Epoch 00038: loss improved from 1.77966 to 1.77850, saving model to word_pred_Model4.h5\n",
      "Epoch 39/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7778 - accuracy: 0.5214\n",
      "\n",
      "Epoch 00039: loss improved from 1.77850 to 1.77776, saving model to word_pred_Model4.h5\n",
      "Epoch 40/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7767 - accuracy: 0.5218\n",
      "\n",
      "Epoch 00040: loss improved from 1.77776 to 1.77668, saving model to word_pred_Model4.h5\n",
      "Epoch 41/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7757 - accuracy: 0.5219\n",
      "\n",
      "Epoch 00041: loss improved from 1.77668 to 1.77568, saving model to word_pred_Model4.h5\n",
      "Epoch 42/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7748 - accuracy: 0.5223\n",
      "\n",
      "Epoch 00042: loss improved from 1.77568 to 1.77482, saving model to word_pred_Model4.h5\n",
      "Epoch 43/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7739 - accuracy: 0.5227\n",
      "\n",
      "Epoch 00043: loss improved from 1.77482 to 1.77394, saving model to word_pred_Model4.h5\n",
      "Epoch 44/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7730 - accuracy: 0.5231\n",
      "\n",
      "Epoch 00044: loss improved from 1.77394 to 1.77302, saving model to word_pred_Model4.h5\n",
      "Epoch 45/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7722 - accuracy: 0.5229\n",
      "\n",
      "Epoch 00045: loss improved from 1.77302 to 1.77216, saving model to word_pred_Model4.h5\n",
      "Epoch 46/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7717 - accuracy: 0.5234\n",
      "\n",
      "Epoch 00046: loss improved from 1.77216 to 1.77169, saving model to word_pred_Model4.h5\n",
      "Epoch 47/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7712 - accuracy: 0.5234\n",
      "\n",
      "Epoch 00047: loss improved from 1.77169 to 1.77118, saving model to word_pred_Model4.h5\n",
      "Epoch 48/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7698 - accuracy: 0.5238\n",
      "\n",
      "Epoch 00048: loss improved from 1.77118 to 1.76982, saving model to word_pred_Model4.h5\n",
      "Epoch 49/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7696 - accuracy: 0.5238\n",
      "\n",
      "Epoch 00049: loss improved from 1.76982 to 1.76963, saving model to word_pred_Model4.h5\n",
      "Epoch 50/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7691 - accuracy: 0.5237\n",
      "\n",
      "Epoch 00050: loss improved from 1.76963 to 1.76905, saving model to word_pred_Model4.h5\n",
      "Epoch 51/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7683 - accuracy: 0.5241\n",
      "\n",
      "Epoch 00051: loss improved from 1.76905 to 1.76833, saving model to word_pred_Model4.h5\n",
      "Epoch 52/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7675 - accuracy: 0.5245\n",
      "\n",
      "Epoch 00052: loss improved from 1.76833 to 1.76749, saving model to word_pred_Model4.h5\n",
      "Epoch 53/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7668 - accuracy: 0.5247\n",
      "\n",
      "Epoch 00053: loss improved from 1.76749 to 1.76681, saving model to word_pred_Model4.h5\n",
      "Epoch 54/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7666 - accuracy: 0.5248\n",
      "\n",
      "Epoch 00054: loss improved from 1.76681 to 1.76658, saving model to word_pred_Model4.h5\n",
      "Epoch 55/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7658 - accuracy: 0.5251\n",
      "\n",
      "Epoch 00055: loss improved from 1.76658 to 1.76576, saving model to word_pred_Model4.h5\n",
      "Epoch 56/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7654 - accuracy: 0.5247\n",
      "\n",
      "Epoch 00056: loss improved from 1.76576 to 1.76544, saving model to word_pred_Model4.h5\n",
      "Epoch 57/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7649 - accuracy: 0.5249\n",
      "\n",
      "Epoch 00057: loss improved from 1.76544 to 1.76489, saving model to word_pred_Model4.h5\n",
      "Epoch 58/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7645 - accuracy: 0.5251\n",
      "\n",
      "Epoch 00058: loss improved from 1.76489 to 1.76448, saving model to word_pred_Model4.h5\n",
      "Epoch 59/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7638 - accuracy: 0.5253\n",
      "\n",
      "Epoch 00059: loss improved from 1.76448 to 1.76382, saving model to word_pred_Model4.h5\n",
      "Epoch 60/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7633 - accuracy: 0.5258\n",
      "\n",
      "Epoch 00060: loss improved from 1.76382 to 1.76327, saving model to word_pred_Model4.h5\n",
      "Epoch 61/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7628 - accuracy: 0.5254\n",
      "\n",
      "Epoch 00061: loss improved from 1.76327 to 1.76276, saving model to word_pred_Model4.h5\n",
      "Epoch 62/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7626 - accuracy: 0.5255\n",
      "\n",
      "Epoch 00062: loss improved from 1.76276 to 1.76259, saving model to word_pred_Model4.h5\n",
      "Epoch 63/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7622 - accuracy: 0.5258\n",
      "\n",
      "Epoch 00063: loss improved from 1.76259 to 1.76215, saving model to word_pred_Model4.h5\n",
      "Epoch 64/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7618 - accuracy: 0.5258\n",
      "\n",
      "Epoch 00064: loss improved from 1.76215 to 1.76176, saving model to word_pred_Model4.h5\n",
      "Epoch 65/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7614 - accuracy: 0.5257\n",
      "\n",
      "Epoch 00065: loss improved from 1.76176 to 1.76143, saving model to word_pred_Model4.h5\n",
      "Epoch 66/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7610 - accuracy: 0.5262\n",
      "\n",
      "Epoch 00066: loss improved from 1.76143 to 1.76101, saving model to word_pred_Model4.h5\n",
      "Epoch 67/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7605 - accuracy: 0.5260\n",
      "\n",
      "Epoch 00067: loss improved from 1.76101 to 1.76052, saving model to word_pred_Model4.h5\n",
      "Epoch 68/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7605 - accuracy: 0.5263\n",
      "\n",
      "Epoch 00068: loss did not improve from 1.76052\n",
      "Epoch 69/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7598 - accuracy: 0.5261\n",
      "\n",
      "Epoch 00069: loss improved from 1.76052 to 1.75975, saving model to word_pred_Model4.h5\n",
      "Epoch 70/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7595 - accuracy: 0.5266\n",
      "\n",
      "Epoch 00070: loss improved from 1.75975 to 1.75951, saving model to word_pred_Model4.h5\n",
      "Epoch 71/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7592 - accuracy: 0.5266\n",
      "\n",
      "Epoch 00071: loss improved from 1.75951 to 1.75919, saving model to word_pred_Model4.h5\n",
      "Epoch 72/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7591 - accuracy: 0.5265\n",
      "\n",
      "Epoch 00072: loss improved from 1.75919 to 1.75907, saving model to word_pred_Model4.h5\n",
      "Epoch 73/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7585 - accuracy: 0.5266\n",
      "\n",
      "Epoch 00073: loss improved from 1.75907 to 1.75851, saving model to word_pred_Model4.h5\n",
      "Epoch 74/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7583 - accuracy: 0.5269\n",
      "\n",
      "Epoch 00074: loss improved from 1.75851 to 1.75830, saving model to word_pred_Model4.h5\n",
      "Epoch 75/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7578 - accuracy: 0.5268\n",
      "\n",
      "Epoch 00075: loss improved from 1.75830 to 1.75783, saving model to word_pred_Model4.h5\n",
      "Epoch 76/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7578 - accuracy: 0.5269\n",
      "\n",
      "Epoch 00076: loss improved from 1.75783 to 1.75775, saving model to word_pred_Model4.h5\n",
      "Epoch 77/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7573 - accuracy: 0.5271\n",
      "\n",
      "Epoch 00077: loss improved from 1.75775 to 1.75725, saving model to word_pred_Model4.h5\n",
      "Epoch 78/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7572 - accuracy: 0.5271\n",
      "\n",
      "Epoch 00078: loss improved from 1.75725 to 1.75720, saving model to word_pred_Model4.h5\n",
      "Epoch 79/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7569 - accuracy: 0.5271\n",
      "\n",
      "Epoch 00079: loss improved from 1.75720 to 1.75686, saving model to word_pred_Model4.h5\n",
      "Epoch 80/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7568 - accuracy: 0.5271\n",
      "\n",
      "Epoch 00080: loss improved from 1.75686 to 1.75680, saving model to word_pred_Model4.h5\n",
      "Epoch 81/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7562 - accuracy: 0.5277\n",
      "\n",
      "Epoch 00081: loss improved from 1.75680 to 1.75624, saving model to word_pred_Model4.h5\n",
      "Epoch 82/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7562 - accuracy: 0.5275\n",
      "\n",
      "Epoch 00082: loss improved from 1.75624 to 1.75618, saving model to word_pred_Model4.h5\n",
      "Epoch 83/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1136924/1136924 [==============================] - 81s 71us/step - loss: 1.7557 - accuracy: 0.5275\n",
      "\n",
      "Epoch 00083: loss improved from 1.75618 to 1.75571, saving model to word_pred_Model4.h5\n",
      "Epoch 84/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7561 - accuracy: 0.5275\n",
      "\n",
      "Epoch 00084: loss did not improve from 1.75571\n",
      "Epoch 85/500\n",
      "1136924/1136924 [==============================] - 81s 71us/step - loss: 1.7554 - accuracy: 0.5276\n",
      "\n",
      "Epoch 00085: loss improved from 1.75571 to 1.75536, saving model to word_pred_Model4.h5\n",
      "Epoch 86/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7550 - accuracy: 0.5276\n",
      "\n",
      "Epoch 00086: loss improved from 1.75536 to 1.75503, saving model to word_pred_Model4.h5\n",
      "Epoch 87/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7550 - accuracy: 0.5278\n",
      "\n",
      "Epoch 00087: loss improved from 1.75503 to 1.75497, saving model to word_pred_Model4.h5\n",
      "Epoch 88/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7549 - accuracy: 0.5277\n",
      "\n",
      "Epoch 00088: loss improved from 1.75497 to 1.75486, saving model to word_pred_Model4.h5\n",
      "Epoch 89/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7547 - accuracy: 0.5278\n",
      "\n",
      "Epoch 00089: loss improved from 1.75486 to 1.75470, saving model to word_pred_Model4.h5\n",
      "Epoch 90/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7546 - accuracy: 0.5281\n",
      "\n",
      "Epoch 00090: loss improved from 1.75470 to 1.75456, saving model to word_pred_Model4.h5\n",
      "Epoch 91/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7544 - accuracy: 0.5279\n",
      "\n",
      "Epoch 00091: loss improved from 1.75456 to 1.75440, saving model to word_pred_Model4.h5\n",
      "Epoch 92/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7541 - accuracy: 0.5281\n",
      "\n",
      "Epoch 00092: loss improved from 1.75440 to 1.75405, saving model to word_pred_Model4.h5\n",
      "Epoch 93/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7540 - accuracy: 0.5281\n",
      "\n",
      "Epoch 00093: loss improved from 1.75405 to 1.75396, saving model to word_pred_Model4.h5\n",
      "Epoch 94/500\n",
      "1136924/1136924 [==============================] - 81s 71us/step - loss: 1.7536 - accuracy: 0.5282\n",
      "\n",
      "Epoch 00094: loss improved from 1.75396 to 1.75364, saving model to word_pred_Model4.h5\n",
      "Epoch 95/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7535 - accuracy: 0.5279\n",
      "\n",
      "Epoch 00095: loss improved from 1.75364 to 1.75347, saving model to word_pred_Model4.h5\n",
      "Epoch 96/500\n",
      "1136924/1136924 [==============================] - 81s 71us/step - loss: 1.7535 - accuracy: 0.5280\n",
      "\n",
      "Epoch 00096: loss did not improve from 1.75347\n",
      "Epoch 97/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7532 - accuracy: 0.5283\n",
      "\n",
      "Epoch 00097: loss improved from 1.75347 to 1.75317, saving model to word_pred_Model4.h5\n",
      "Epoch 98/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7532 - accuracy: 0.5280\n",
      "\n",
      "Epoch 00098: loss did not improve from 1.75317\n",
      "Epoch 99/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7526 - accuracy: 0.5282\n",
      "\n",
      "Epoch 00099: loss improved from 1.75317 to 1.75257, saving model to word_pred_Model4.h5\n",
      "Epoch 100/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7526 - accuracy: 0.5281\n",
      "\n",
      "Epoch 00100: loss did not improve from 1.75257\n",
      "Epoch 101/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7523 - accuracy: 0.5287\n",
      "\n",
      "Epoch 00101: loss improved from 1.75257 to 1.75232, saving model to word_pred_Model4.h5\n",
      "Epoch 102/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7522 - accuracy: 0.5283\n",
      "\n",
      "Epoch 00102: loss improved from 1.75232 to 1.75225, saving model to word_pred_Model4.h5\n",
      "Epoch 103/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7522 - accuracy: 0.5283\n",
      "\n",
      "Epoch 00103: loss improved from 1.75225 to 1.75224, saving model to word_pred_Model4.h5\n",
      "Epoch 104/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7521 - accuracy: 0.5286\n",
      "\n",
      "Epoch 00104: loss improved from 1.75224 to 1.75214, saving model to word_pred_Model4.h5\n",
      "Epoch 105/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7517 - accuracy: 0.5288\n",
      "\n",
      "Epoch 00105: loss improved from 1.75214 to 1.75169, saving model to word_pred_Model4.h5\n",
      "Epoch 106/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7518 - accuracy: 0.5286\n",
      "\n",
      "Epoch 00106: loss did not improve from 1.75169\n",
      "Epoch 107/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7518 - accuracy: 0.5290\n",
      "\n",
      "Epoch 00107: loss did not improve from 1.75169\n",
      "Epoch 108/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7518 - accuracy: 0.5288\n",
      "\n",
      "Epoch 00108: loss did not improve from 1.75169\n",
      "Epoch 109/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7518 - accuracy: 0.5287\n",
      "\n",
      "Epoch 00109: loss did not improve from 1.75169\n",
      "Epoch 110/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7511 - accuracy: 0.5287\n",
      "\n",
      "Epoch 00110: loss improved from 1.75169 to 1.75114, saving model to word_pred_Model4.h5\n",
      "Epoch 111/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7513 - accuracy: 0.5287\n",
      "\n",
      "Epoch 00111: loss did not improve from 1.75114\n",
      "Epoch 112/500\n",
      "1136924/1136924 [==============================] - 82s 72us/step - loss: 1.7513 - accuracy: 0.5287\n",
      "\n",
      "Epoch 00112: loss did not improve from 1.75114\n",
      "Epoch 113/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7511 - accuracy: 0.5290\n",
      "\n",
      "Epoch 00113: loss improved from 1.75114 to 1.75105, saving model to word_pred_Model4.h5\n",
      "Epoch 114/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7510 - accuracy: 0.5288\n",
      "\n",
      "Epoch 00114: loss improved from 1.75105 to 1.75099, saving model to word_pred_Model4.h5\n",
      "Epoch 115/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7509 - accuracy: 0.5290\n",
      "\n",
      "Epoch 00115: loss improved from 1.75099 to 1.75089, saving model to word_pred_Model4.h5\n",
      "Epoch 116/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7505 - accuracy: 0.5292\n",
      "\n",
      "Epoch 00116: loss improved from 1.75089 to 1.75048, saving model to word_pred_Model4.h5\n",
      "Epoch 117/500\n",
      "1136924/1136924 [==============================] - 81s 71us/step - loss: 1.7508 - accuracy: 0.5288\n",
      "\n",
      "Epoch 00117: loss did not improve from 1.75048\n",
      "Epoch 118/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7506 - accuracy: 0.5290\n",
      "\n",
      "Epoch 00118: loss did not improve from 1.75048\n",
      "Epoch 119/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7505 - accuracy: 0.5287\n",
      "\n",
      "Epoch 00119: loss did not improve from 1.75048\n",
      "Epoch 120/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7504 - accuracy: 0.5292\n",
      "\n",
      "Epoch 00120: loss improved from 1.75048 to 1.75039, saving model to word_pred_Model4.h5\n",
      "Epoch 121/500\n",
      "1136924/1136924 [==============================] - 81s 71us/step - loss: 1.7503 - accuracy: 0.5288\n",
      "\n",
      "Epoch 00121: loss improved from 1.75039 to 1.75028, saving model to word_pred_Model4.h5\n",
      "Epoch 122/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7500 - accuracy: 0.5292\n",
      "\n",
      "Epoch 00122: loss improved from 1.75028 to 1.74997, saving model to word_pred_Model4.h5\n",
      "Epoch 123/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7500 - accuracy: 0.5291\n",
      "\n",
      "Epoch 00123: loss did not improve from 1.74997\n",
      "Epoch 124/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7499 - accuracy: 0.5292\n",
      "\n",
      "Epoch 00124: loss improved from 1.74997 to 1.74993, saving model to word_pred_Model4.h5\n",
      "Epoch 125/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7499 - accuracy: 0.5293\n",
      "\n",
      "Epoch 00125: loss improved from 1.74993 to 1.74993, saving model to word_pred_Model4.h5\n",
      "Epoch 126/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7495 - accuracy: 0.5294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00126: loss improved from 1.74993 to 1.74951, saving model to word_pred_Model4.h5\n",
      "Epoch 127/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7496 - accuracy: 0.5292\n",
      "\n",
      "Epoch 00127: loss did not improve from 1.74951\n",
      "Epoch 128/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7494 - accuracy: 0.5291\n",
      "\n",
      "Epoch 00128: loss improved from 1.74951 to 1.74941, saving model to word_pred_Model4.h5\n",
      "Epoch 129/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7495 - accuracy: 0.5295\n",
      "\n",
      "Epoch 00129: loss did not improve from 1.74941\n",
      "Epoch 130/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7493 - accuracy: 0.5295\n",
      "\n",
      "Epoch 00130: loss improved from 1.74941 to 1.74930, saving model to word_pred_Model4.h5\n",
      "Epoch 131/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7496 - accuracy: 0.5294\n",
      "\n",
      "Epoch 00131: loss did not improve from 1.74930\n",
      "Epoch 132/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7492 - accuracy: 0.5293\n",
      "\n",
      "Epoch 00132: loss improved from 1.74930 to 1.74921, saving model to word_pred_Model4.h5\n",
      "Epoch 133/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7491 - accuracy: 0.5293\n",
      "\n",
      "Epoch 00133: loss improved from 1.74921 to 1.74907, saving model to word_pred_Model4.h5\n",
      "Epoch 134/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7493 - accuracy: 0.5296\n",
      "\n",
      "Epoch 00134: loss did not improve from 1.74907\n",
      "Epoch 135/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7489 - accuracy: 0.5294\n",
      "\n",
      "Epoch 00135: loss improved from 1.74907 to 1.74887, saving model to word_pred_Model4.h5\n",
      "Epoch 136/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7491 - accuracy: 0.5298\n",
      "\n",
      "Epoch 00136: loss did not improve from 1.74887\n",
      "Epoch 137/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7489 - accuracy: 0.5295\n",
      "\n",
      "Epoch 00137: loss did not improve from 1.74887\n",
      "Epoch 138/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7486 - accuracy: 0.5297\n",
      "\n",
      "Epoch 00138: loss improved from 1.74887 to 1.74861, saving model to word_pred_Model4.h5\n",
      "Epoch 139/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7488 - accuracy: 0.5296\n",
      "\n",
      "Epoch 00139: loss did not improve from 1.74861\n",
      "Epoch 140/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7486 - accuracy: 0.5292\n",
      "\n",
      "Epoch 00140: loss improved from 1.74861 to 1.74857, saving model to word_pred_Model4.h5\n",
      "Epoch 141/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7485 - accuracy: 0.5297\n",
      "\n",
      "Epoch 00141: loss improved from 1.74857 to 1.74853, saving model to word_pred_Model4.h5\n",
      "Epoch 142/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7487 - accuracy: 0.5297\n",
      "\n",
      "Epoch 00142: loss did not improve from 1.74853\n",
      "Epoch 143/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7482 - accuracy: 0.5296\n",
      "\n",
      "Epoch 00143: loss improved from 1.74853 to 1.74820, saving model to word_pred_Model4.h5\n",
      "Epoch 144/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7486 - accuracy: 0.5297\n",
      "\n",
      "Epoch 00144: loss did not improve from 1.74820\n",
      "Epoch 145/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7484 - accuracy: 0.5296\n",
      "\n",
      "Epoch 00145: loss did not improve from 1.74820\n",
      "Epoch 146/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7479 - accuracy: 0.5298\n",
      "\n",
      "Epoch 00146: loss improved from 1.74820 to 1.74792, saving model to word_pred_Model4.h5\n",
      "Epoch 147/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7482 - accuracy: 0.5297\n",
      "\n",
      "Epoch 00147: loss did not improve from 1.74792\n",
      "Epoch 148/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7487 - accuracy: 0.5297\n",
      "\n",
      "Epoch 00148: loss did not improve from 1.74792\n",
      "Epoch 149/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7480 - accuracy: 0.5296\n",
      "\n",
      "Epoch 00149: loss did not improve from 1.74792\n",
      "Epoch 150/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7481 - accuracy: 0.5300\n",
      "\n",
      "Epoch 00150: loss did not improve from 1.74792\n",
      "Epoch 151/500\n",
      "1136924/1136924 [==============================] - 81s 71us/step - loss: 1.7479 - accuracy: 0.5294\n",
      "\n",
      "Epoch 00151: loss improved from 1.74792 to 1.74790, saving model to word_pred_Model4.h5\n",
      "Epoch 152/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7476 - accuracy: 0.5296\n",
      "\n",
      "Epoch 00152: loss improved from 1.74790 to 1.74760, saving model to word_pred_Model4.h5\n",
      "Epoch 153/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7476 - accuracy: 0.5302\n",
      "\n",
      "Epoch 00153: loss improved from 1.74760 to 1.74757, saving model to word_pred_Model4.h5\n",
      "Epoch 154/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7479 - accuracy: 0.5301\n",
      "\n",
      "Epoch 00154: loss did not improve from 1.74757\n",
      "Epoch 155/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7483 - accuracy: 0.5299\n",
      "\n",
      "Epoch 00155: loss did not improve from 1.74757\n",
      "Epoch 156/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7478 - accuracy: 0.5299\n",
      "\n",
      "Epoch 00156: loss did not improve from 1.74757\n",
      "Epoch 157/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7482 - accuracy: 0.5296\n",
      "\n",
      "Epoch 00157: loss did not improve from 1.74757\n",
      "Epoch 158/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7477 - accuracy: 0.5295\n",
      "\n",
      "Epoch 00158: loss did not improve from 1.74757\n",
      "Epoch 159/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7478 - accuracy: 0.5301\n",
      "\n",
      "Epoch 00159: loss did not improve from 1.74757\n",
      "Epoch 160/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7474 - accuracy: 0.5300\n",
      "\n",
      "Epoch 00160: loss improved from 1.74757 to 1.74740, saving model to word_pred_Model4.h5\n",
      "Epoch 161/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7476 - accuracy: 0.5302\n",
      "\n",
      "Epoch 00161: loss did not improve from 1.74740\n",
      "Epoch 162/500\n",
      "1136924/1136924 [==============================] - 81s 71us/step - loss: 1.7477 - accuracy: 0.5299\n",
      "\n",
      "Epoch 00162: loss did not improve from 1.74740\n",
      "Epoch 163/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7475 - accuracy: 0.5299\n",
      "\n",
      "Epoch 00163: loss did not improve from 1.74740\n",
      "Epoch 164/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7474 - accuracy: 0.5298\n",
      "\n",
      "Epoch 00164: loss improved from 1.74740 to 1.74737, saving model to word_pred_Model4.h5\n",
      "Epoch 165/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7473 - accuracy: 0.5301\n",
      "\n",
      "Epoch 00165: loss improved from 1.74737 to 1.74733, saving model to word_pred_Model4.h5\n",
      "Epoch 166/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7478 - accuracy: 0.5297\n",
      "\n",
      "Epoch 00166: loss did not improve from 1.74733\n",
      "Epoch 167/500\n",
      "1136924/1136924 [==============================] - 84s 74us/step - loss: 1.7476 - accuracy: 0.5298\n",
      "\n",
      "Epoch 00167: loss did not improve from 1.74733\n",
      "Epoch 168/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7473 - accuracy: 0.5300\n",
      "\n",
      "Epoch 00168: loss improved from 1.74733 to 1.74730, saving model to word_pred_Model4.h5\n",
      "Epoch 169/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7473 - accuracy: 0.5302\n",
      "\n",
      "Epoch 00169: loss did not improve from 1.74730\n",
      "Epoch 170/500\n",
      "1136924/1136924 [==============================] - 79s 70us/step - loss: 1.7470 - accuracy: 0.5303\n",
      "\n",
      "Epoch 00170: loss improved from 1.74730 to 1.74701, saving model to word_pred_Model4.h5\n",
      "Epoch 171/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7474 - accuracy: 0.5301\n",
      "\n",
      "Epoch 00171: loss did not improve from 1.74701\n",
      "Epoch 172/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7472 - accuracy: 0.5301\n",
      "\n",
      "Epoch 00172: loss did not improve from 1.74701\n",
      "Epoch 173/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1136924/1136924 [==============================] - 81s 71us/step - loss: 1.7475 - accuracy: 0.5297\n",
      "\n",
      "Epoch 00173: loss did not improve from 1.74701\n",
      "Epoch 174/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7473 - accuracy: 0.5303\n",
      "\n",
      "Epoch 00174: loss did not improve from 1.74701\n",
      "Epoch 175/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7471 - accuracy: 0.5301\n",
      "\n",
      "Epoch 00175: loss did not improve from 1.74701\n",
      "Epoch 176/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7467 - accuracy: 0.5301\n",
      "\n",
      "Epoch 00176: loss improved from 1.74701 to 1.74667, saving model to word_pred_Model4.h5\n",
      "Epoch 177/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7474 - accuracy: 0.5301\n",
      "\n",
      "Epoch 00177: loss did not improve from 1.74667\n",
      "Epoch 178/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7468 - accuracy: 0.5302\n",
      "\n",
      "Epoch 00178: loss did not improve from 1.74667\n",
      "Epoch 179/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7469 - accuracy: 0.5300\n",
      "\n",
      "Epoch 00179: loss did not improve from 1.74667\n",
      "Epoch 180/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7467 - accuracy: 0.5300\n",
      "\n",
      "Epoch 00180: loss did not improve from 1.74667\n",
      "Epoch 181/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7469 - accuracy: 0.5300\n",
      "\n",
      "Epoch 00181: loss did not improve from 1.74667\n",
      "Epoch 182/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7468 - accuracy: 0.5303\n",
      "\n",
      "Epoch 00182: loss did not improve from 1.74667\n",
      "Epoch 183/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7471 - accuracy: 0.5300\n",
      "\n",
      "Epoch 00183: loss did not improve from 1.74667\n",
      "Epoch 184/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7468 - accuracy: 0.5301\n",
      "\n",
      "Epoch 00184: loss did not improve from 1.74667\n",
      "Epoch 185/500\n",
      "1136924/1136924 [==============================] - 81s 71us/step - loss: 1.7472 - accuracy: 0.5301\n",
      "\n",
      "Epoch 00185: loss did not improve from 1.74667\n",
      "Epoch 186/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7468 - accuracy: 0.5302\n",
      "\n",
      "Epoch 00186: loss did not improve from 1.74667\n",
      "Epoch 187/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7462 - accuracy: 0.5303\n",
      "\n",
      "Epoch 00187: loss improved from 1.74667 to 1.74624, saving model to word_pred_Model4.h5\n",
      "Epoch 188/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7469 - accuracy: 0.5300\n",
      "\n",
      "Epoch 00188: loss did not improve from 1.74624\n",
      "Epoch 189/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7465 - accuracy: 0.5303\n",
      "\n",
      "Epoch 00189: loss did not improve from 1.74624\n",
      "Epoch 190/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7465 - accuracy: 0.5303\n",
      "\n",
      "Epoch 00190: loss did not improve from 1.74624\n",
      "Epoch 191/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7465 - accuracy: 0.5303\n",
      "\n",
      "Epoch 00191: loss did not improve from 1.74624\n",
      "Epoch 192/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7462 - accuracy: 0.5303\n",
      "\n",
      "Epoch 00192: loss improved from 1.74624 to 1.74624, saving model to word_pred_Model4.h5\n",
      "Epoch 193/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7465 - accuracy: 0.5304\n",
      "\n",
      "Epoch 00193: loss did not improve from 1.74624\n",
      "Epoch 194/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7468 - accuracy: 0.5304\n",
      "\n",
      "Epoch 00194: loss did not improve from 1.74624\n",
      "Epoch 195/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7466 - accuracy: 0.5303\n",
      "\n",
      "Epoch 00195: loss did not improve from 1.74624\n",
      "Epoch 196/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7472 - accuracy: 0.5301\n",
      "\n",
      "Epoch 00196: loss did not improve from 1.74624\n",
      "Epoch 197/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7463 - accuracy: 0.5303\n",
      "\n",
      "Epoch 00197: loss did not improve from 1.74624\n",
      "Epoch 198/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7463 - accuracy: 0.5305\n",
      "\n",
      "Epoch 00198: loss did not improve from 1.74624\n",
      "Epoch 199/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7466 - accuracy: 0.5302\n",
      "\n",
      "Epoch 00199: loss did not improve from 1.74624\n",
      "Epoch 200/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7468 - accuracy: 0.5304\n",
      "\n",
      "Epoch 00200: loss did not improve from 1.74624\n",
      "Epoch 201/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7467 - accuracy: 0.5303\n",
      "\n",
      "Epoch 00201: loss did not improve from 1.74624\n",
      "Epoch 202/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7463 - accuracy: 0.5303\n",
      "\n",
      "Epoch 00202: loss did not improve from 1.74624\n",
      "Epoch 203/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7465 - accuracy: 0.5305\n",
      "\n",
      "Epoch 00203: loss did not improve from 1.74624\n",
      "Epoch 204/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7460 - accuracy: 0.5303\n",
      "\n",
      "Epoch 00204: loss improved from 1.74624 to 1.74605, saving model to word_pred_Model4.h5\n",
      "Epoch 205/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7466 - accuracy: 0.5306\n",
      "\n",
      "Epoch 00205: loss did not improve from 1.74605\n",
      "Epoch 206/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7462 - accuracy: 0.5304\n",
      "\n",
      "Epoch 00206: loss did not improve from 1.74605\n",
      "Epoch 207/500\n",
      "1136924/1136924 [==============================] - 81s 71us/step - loss: 1.7466 - accuracy: 0.5304\n",
      "\n",
      "Epoch 00207: loss did not improve from 1.74605\n",
      "Epoch 208/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7463 - accuracy: 0.5303\n",
      "\n",
      "Epoch 00208: loss did not improve from 1.74605\n",
      "Epoch 209/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7464 - accuracy: 0.5302\n",
      "\n",
      "Epoch 00209: loss did not improve from 1.74605\n",
      "Epoch 210/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7464 - accuracy: 0.5302\n",
      "\n",
      "Epoch 00210: loss did not improve from 1.74605\n",
      "Epoch 211/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7463 - accuracy: 0.5303\n",
      "\n",
      "Epoch 00211: loss did not improve from 1.74605\n",
      "Epoch 212/500\n",
      "1136924/1136924 [==============================] - 81s 71us/step - loss: 1.7465 - accuracy: 0.5308\n",
      "\n",
      "Epoch 00212: loss did not improve from 1.74605\n",
      "Epoch 213/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7466 - accuracy: 0.5302\n",
      "\n",
      "Epoch 00213: loss did not improve from 1.74605\n",
      "Epoch 214/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7464 - accuracy: 0.5306\n",
      "\n",
      "Epoch 00214: loss did not improve from 1.74605\n",
      "Epoch 215/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7466 - accuracy: 0.5302\n",
      "\n",
      "Epoch 00215: loss did not improve from 1.74605\n",
      "Epoch 216/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7461 - accuracy: 0.5305\n",
      "\n",
      "Epoch 00216: loss did not improve from 1.74605\n",
      "Epoch 217/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7464 - accuracy: 0.5308\n",
      "\n",
      "Epoch 00217: loss did not improve from 1.74605\n",
      "Epoch 218/500\n",
      "1136924/1136924 [==============================] - 81s 71us/step - loss: 1.7463 - accuracy: 0.5307\n",
      "\n",
      "Epoch 00218: loss did not improve from 1.74605\n",
      "Epoch 219/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7464 - accuracy: 0.5302\n",
      "\n",
      "Epoch 00219: loss did not improve from 1.74605\n",
      "Epoch 220/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7457 - accuracy: 0.5306\n",
      "\n",
      "Epoch 00220: loss improved from 1.74605 to 1.74572, saving model to word_pred_Model4.h5\n",
      "Epoch 221/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7460 - accuracy: 0.5305\n",
      "\n",
      "Epoch 00221: loss did not improve from 1.74572\n",
      "Epoch 222/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7463 - accuracy: 0.5300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00222: loss did not improve from 1.74572\n",
      "Epoch 223/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7462 - accuracy: 0.5308\n",
      "\n",
      "Epoch 00223: loss did not improve from 1.74572\n",
      "Epoch 224/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7458 - accuracy: 0.5305\n",
      "\n",
      "Epoch 00224: loss did not improve from 1.74572\n",
      "Epoch 225/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7464 - accuracy: 0.5303\n",
      "\n",
      "Epoch 00225: loss did not improve from 1.74572\n",
      "Epoch 226/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7461 - accuracy: 0.5305\n",
      "\n",
      "Epoch 00226: loss did not improve from 1.74572\n",
      "Epoch 227/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7461 - accuracy: 0.5307\n",
      "\n",
      "Epoch 00227: loss did not improve from 1.74572\n",
      "Epoch 228/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7463 - accuracy: 0.5305\n",
      "\n",
      "Epoch 00228: loss did not improve from 1.74572\n",
      "Epoch 229/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7466 - accuracy: 0.5302\n",
      "\n",
      "Epoch 00229: loss did not improve from 1.74572\n",
      "Epoch 230/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7459 - accuracy: 0.5304\n",
      "\n",
      "Epoch 00230: loss did not improve from 1.74572\n",
      "Epoch 231/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7462 - accuracy: 0.5307\n",
      "\n",
      "Epoch 00231: loss did not improve from 1.74572\n",
      "Epoch 232/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7457 - accuracy: 0.5308\n",
      "\n",
      "Epoch 00232: loss improved from 1.74572 to 1.74572, saving model to word_pred_Model4.h5\n",
      "Epoch 233/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7459 - accuracy: 0.5305\n",
      "\n",
      "Epoch 00233: loss did not improve from 1.74572\n",
      "Epoch 234/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7461 - accuracy: 0.5306\n",
      "\n",
      "Epoch 00234: loss did not improve from 1.74572\n",
      "Epoch 235/500\n",
      "1136924/1136924 [==============================] - 81s 71us/step - loss: 1.7462 - accuracy: 0.5302\n",
      "\n",
      "Epoch 00235: loss did not improve from 1.74572\n",
      "Epoch 236/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7460 - accuracy: 0.5302\n",
      "\n",
      "Epoch 00236: loss did not improve from 1.74572\n",
      "Epoch 237/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7458 - accuracy: 0.5303\n",
      "\n",
      "Epoch 00237: loss did not improve from 1.74572\n",
      "Epoch 238/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7462 - accuracy: 0.5309\n",
      "\n",
      "Epoch 00238: loss did not improve from 1.74572\n",
      "Epoch 239/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7461 - accuracy: 0.5304\n",
      "\n",
      "Epoch 00239: loss did not improve from 1.74572\n",
      "Epoch 240/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7459 - accuracy: 0.5302\n",
      "\n",
      "Epoch 00240: loss did not improve from 1.74572\n",
      "Epoch 241/500\n",
      "1136924/1136924 [==============================] - 81s 71us/step - loss: 1.7460 - accuracy: 0.5306\n",
      "\n",
      "Epoch 00241: loss did not improve from 1.74572\n",
      "Epoch 242/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7463 - accuracy: 0.5301\n",
      "\n",
      "Epoch 00242: loss did not improve from 1.74572\n",
      "Epoch 243/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7458 - accuracy: 0.5305\n",
      "\n",
      "Epoch 00243: loss did not improve from 1.74572\n",
      "Epoch 244/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7463 - accuracy: 0.5302\n",
      "\n",
      "Epoch 00244: loss did not improve from 1.74572\n",
      "Epoch 245/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7460 - accuracy: 0.5304\n",
      "\n",
      "Epoch 00245: loss did not improve from 1.74572\n",
      "Epoch 246/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7458 - accuracy: 0.5306\n",
      "\n",
      "Epoch 00246: loss did not improve from 1.74572\n",
      "Epoch 247/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7458 - accuracy: 0.5308\n",
      "\n",
      "Epoch 00247: loss did not improve from 1.74572\n",
      "Epoch 248/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7457 - accuracy: 0.5306\n",
      "\n",
      "Epoch 00248: loss improved from 1.74572 to 1.74570, saving model to word_pred_Model4.h5\n",
      "Epoch 249/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7462 - accuracy: 0.5305\n",
      "\n",
      "Epoch 00249: loss did not improve from 1.74570\n",
      "Epoch 250/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7461 - accuracy: 0.5302\n",
      "\n",
      "Epoch 00250: loss did not improve from 1.74570\n",
      "Epoch 251/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7461 - accuracy: 0.5310\n",
      "\n",
      "Epoch 00251: loss did not improve from 1.74570\n",
      "Epoch 252/500\n",
      "1136924/1136924 [==============================] - 81s 71us/step - loss: 1.7460 - accuracy: 0.5308\n",
      "\n",
      "Epoch 00252: loss did not improve from 1.74570\n",
      "Epoch 253/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7462 - accuracy: 0.5302\n",
      "\n",
      "Epoch 00253: loss did not improve from 1.74570\n",
      "Epoch 254/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7458 - accuracy: 0.5303\n",
      "\n",
      "Epoch 00254: loss did not improve from 1.74570\n",
      "Epoch 255/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7458 - accuracy: 0.5303\n",
      "\n",
      "Epoch 00255: loss did not improve from 1.74570\n",
      "Epoch 256/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7460 - accuracy: 0.5303\n",
      "\n",
      "Epoch 00256: loss did not improve from 1.74570\n",
      "Epoch 257/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7460 - accuracy: 0.5306\n",
      "\n",
      "Epoch 00257: loss did not improve from 1.74570\n",
      "Epoch 258/500\n",
      "1136924/1136924 [==============================] - 81s 71us/step - loss: 1.7464 - accuracy: 0.5305\n",
      "\n",
      "Epoch 00258: loss did not improve from 1.74570\n",
      "Epoch 259/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7462 - accuracy: 0.5309\n",
      "\n",
      "Epoch 00259: loss did not improve from 1.74570\n",
      "Epoch 260/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7457 - accuracy: 0.5304\n",
      "\n",
      "Epoch 00260: loss improved from 1.74570 to 1.74566, saving model to word_pred_Model4.h5\n",
      "Epoch 261/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7460 - accuracy: 0.5304\n",
      "\n",
      "Epoch 00261: loss did not improve from 1.74566\n",
      "Epoch 262/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7455 - accuracy: 0.5306\n",
      "\n",
      "Epoch 00262: loss improved from 1.74566 to 1.74552, saving model to word_pred_Model4.h5\n",
      "Epoch 263/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7462 - accuracy: 0.5305\n",
      "\n",
      "Epoch 00263: loss did not improve from 1.74552\n",
      "Epoch 264/500\n",
      "1136924/1136924 [==============================] - 81s 71us/step - loss: 1.7462 - accuracy: 0.5302\n",
      "\n",
      "Epoch 00264: loss did not improve from 1.74552\n",
      "Epoch 265/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7457 - accuracy: 0.5306\n",
      "\n",
      "Epoch 00265: loss did not improve from 1.74552\n",
      "Epoch 266/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7457 - accuracy: 0.5305\n",
      "\n",
      "Epoch 00266: loss did not improve from 1.74552\n",
      "Epoch 267/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7466 - accuracy: 0.5305\n",
      "\n",
      "Epoch 00267: loss did not improve from 1.74552\n",
      "Epoch 268/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7463 - accuracy: 0.5303\n",
      "\n",
      "Epoch 00268: loss did not improve from 1.74552\n",
      "Epoch 269/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7453 - accuracy: 0.5308\n",
      "\n",
      "Epoch 00269: loss improved from 1.74552 to 1.74533, saving model to word_pred_Model4.h5\n",
      "Epoch 270/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7458 - accuracy: 0.5305\n",
      "\n",
      "Epoch 00270: loss did not improve from 1.74533\n",
      "Epoch 271/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7460 - accuracy: 0.5309\n",
      "\n",
      "Epoch 00271: loss did not improve from 1.74533\n",
      "Epoch 272/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7456 - accuracy: 0.5312\n",
      "\n",
      "Epoch 00272: loss did not improve from 1.74533\n",
      "Epoch 273/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7463 - accuracy: 0.5308\n",
      "\n",
      "Epoch 00273: loss did not improve from 1.74533\n",
      "Epoch 274/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7457 - accuracy: 0.5309\n",
      "\n",
      "Epoch 00274: loss did not improve from 1.74533\n",
      "Epoch 275/500\n",
      "1136924/1136924 [==============================] - 81s 71us/step - loss: 1.7459 - accuracy: 0.5308\n",
      "\n",
      "Epoch 00275: loss did not improve from 1.74533\n",
      "Epoch 276/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7461 - accuracy: 0.5305\n",
      "\n",
      "Epoch 00276: loss did not improve from 1.74533\n",
      "Epoch 277/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7460 - accuracy: 0.5309\n",
      "\n",
      "Epoch 00277: loss did not improve from 1.74533\n",
      "Epoch 278/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7460 - accuracy: 0.5310\n",
      "\n",
      "Epoch 00278: loss did not improve from 1.74533\n",
      "Epoch 279/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7460 - accuracy: 0.5310\n",
      "\n",
      "Epoch 00279: loss did not improve from 1.74533\n",
      "Epoch 280/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7464 - accuracy: 0.5306\n",
      "\n",
      "Epoch 00280: loss did not improve from 1.74533\n",
      "Epoch 281/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7456 - accuracy: 0.5308\n",
      "\n",
      "Epoch 00281: loss did not improve from 1.74533\n",
      "Epoch 282/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7465 - accuracy: 0.5300\n",
      "\n",
      "Epoch 00282: loss did not improve from 1.74533\n",
      "Epoch 283/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7464 - accuracy: 0.5305\n",
      "\n",
      "Epoch 00283: loss did not improve from 1.74533\n",
      "Epoch 284/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7456 - accuracy: 0.5303\n",
      "\n",
      "Epoch 00284: loss did not improve from 1.74533\n",
      "Epoch 285/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7464 - accuracy: 0.5304\n",
      "\n",
      "Epoch 00285: loss did not improve from 1.74533\n",
      "Epoch 286/500\n",
      "1136924/1136924 [==============================] - 81s 71us/step - loss: 1.7461 - accuracy: 0.5303\n",
      "\n",
      "Epoch 00286: loss did not improve from 1.74533\n",
      "Epoch 287/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7455 - accuracy: 0.5305\n",
      "\n",
      "Epoch 00287: loss did not improve from 1.74533\n",
      "Epoch 288/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7460 - accuracy: 0.5308\n",
      "\n",
      "Epoch 00288: loss did not improve from 1.74533\n",
      "Epoch 289/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7462 - accuracy: 0.5308\n",
      "\n",
      "Epoch 00289: loss did not improve from 1.74533\n",
      "Epoch 290/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7461 - accuracy: 0.5307\n",
      "\n",
      "Epoch 00290: loss did not improve from 1.74533\n",
      "Epoch 291/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7461 - accuracy: 0.5307\n",
      "\n",
      "Epoch 00291: loss did not improve from 1.74533\n",
      "Epoch 292/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7459 - accuracy: 0.5304\n",
      "\n",
      "Epoch 00292: loss did not improve from 1.74533\n",
      "Epoch 293/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7454 - accuracy: 0.5310\n",
      "\n",
      "Epoch 00293: loss did not improve from 1.74533\n",
      "Epoch 294/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7459 - accuracy: 0.5306\n",
      "\n",
      "Epoch 00294: loss did not improve from 1.74533\n",
      "Epoch 295/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7460 - accuracy: 0.5307\n",
      "\n",
      "Epoch 00295: loss did not improve from 1.74533\n",
      "Epoch 296/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7463 - accuracy: 0.5306\n",
      "\n",
      "Epoch 00296: loss did not improve from 1.74533\n",
      "Epoch 297/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7460 - accuracy: 0.5311\n",
      "\n",
      "Epoch 00297: loss did not improve from 1.74533\n",
      "Epoch 298/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7463 - accuracy: 0.5305\n",
      "\n",
      "Epoch 00298: loss did not improve from 1.74533\n",
      "Epoch 299/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7464 - accuracy: 0.5309\n",
      "\n",
      "Epoch 00299: loss did not improve from 1.74533\n",
      "Epoch 300/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7462 - accuracy: 0.5308\n",
      "\n",
      "Epoch 00300: loss did not improve from 1.74533\n",
      "Epoch 301/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7462 - accuracy: 0.5308\n",
      "\n",
      "Epoch 00301: loss did not improve from 1.74533\n",
      "Epoch 302/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7461 - accuracy: 0.5308\n",
      "\n",
      "Epoch 00302: loss did not improve from 1.74533\n",
      "Epoch 303/500\n",
      "1136924/1136924 [==============================] - 81s 71us/step - loss: 1.7465 - accuracy: 0.5308\n",
      "\n",
      "Epoch 00303: loss did not improve from 1.74533\n",
      "Epoch 304/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7458 - accuracy: 0.5308\n",
      "\n",
      "Epoch 00304: loss did not improve from 1.74533\n",
      "Epoch 305/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7461 - accuracy: 0.5304\n",
      "\n",
      "Epoch 00305: loss did not improve from 1.74533\n",
      "Epoch 306/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7464 - accuracy: 0.5306\n",
      "\n",
      "Epoch 00306: loss did not improve from 1.74533\n",
      "Epoch 307/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7462 - accuracy: 0.5305\n",
      "\n",
      "Epoch 00307: loss did not improve from 1.74533\n",
      "Epoch 308/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7463 - accuracy: 0.5307\n",
      "\n",
      "Epoch 00308: loss did not improve from 1.74533\n",
      "Epoch 309/500\n",
      "1136924/1136924 [==============================] - 81s 71us/step - loss: 1.7460 - accuracy: 0.5307\n",
      "\n",
      "Epoch 00309: loss did not improve from 1.74533\n",
      "Epoch 310/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7457 - accuracy: 0.5310\n",
      "\n",
      "Epoch 00310: loss did not improve from 1.74533\n",
      "Epoch 311/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7461 - accuracy: 0.5305\n",
      "\n",
      "Epoch 00311: loss did not improve from 1.74533\n",
      "Epoch 312/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7461 - accuracy: 0.5307\n",
      "\n",
      "Epoch 00312: loss did not improve from 1.74533\n",
      "Epoch 313/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7462 - accuracy: 0.5306\n",
      "\n",
      "Epoch 00313: loss did not improve from 1.74533\n",
      "Epoch 314/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7459 - accuracy: 0.5309\n",
      "\n",
      "Epoch 00314: loss did not improve from 1.74533\n",
      "Epoch 315/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7463 - accuracy: 0.5305\n",
      "\n",
      "Epoch 00315: loss did not improve from 1.74533\n",
      "Epoch 316/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7461 - accuracy: 0.5309\n",
      "\n",
      "Epoch 00316: loss did not improve from 1.74533\n",
      "Epoch 317/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7464 - accuracy: 0.5306\n",
      "\n",
      "Epoch 00317: loss did not improve from 1.74533\n",
      "Epoch 318/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7465 - accuracy: 0.5304\n",
      "\n",
      "Epoch 00318: loss did not improve from 1.74533\n",
      "Epoch 319/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7459 - accuracy: 0.5309\n",
      "\n",
      "Epoch 00319: loss did not improve from 1.74533\n",
      "Epoch 320/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7460 - accuracy: 0.5306\n",
      "\n",
      "Epoch 00320: loss did not improve from 1.74533\n",
      "Epoch 321/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7462 - accuracy: 0.5304\n",
      "\n",
      "Epoch 00321: loss did not improve from 1.74533\n",
      "Epoch 322/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7461 - accuracy: 0.5308\n",
      "\n",
      "Epoch 00322: loss did not improve from 1.74533\n",
      "Epoch 323/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7462 - accuracy: 0.5308\n",
      "\n",
      "Epoch 00323: loss did not improve from 1.74533\n",
      "Epoch 324/500\n",
      "1136924/1136924 [==============================] - 86s 76us/step - loss: 1.7466 - accuracy: 0.5308\n",
      "\n",
      "Epoch 00324: loss did not improve from 1.74533\n",
      "Epoch 325/500\n",
      "1136924/1136924 [==============================] - 92s 81us/step - loss: 1.7463 - accuracy: 0.5309\n",
      "\n",
      "Epoch 00325: loss did not improve from 1.74533\n",
      "Epoch 326/500\n",
      "1136924/1136924 [==============================] - 90s 79us/step - loss: 1.7459 - accuracy: 0.5308\n",
      "\n",
      "Epoch 00326: loss did not improve from 1.74533\n",
      "Epoch 327/500\n",
      "1136924/1136924 [==============================] - 89s 78us/step - loss: 1.7463 - accuracy: 0.5307\n",
      "\n",
      "Epoch 00327: loss did not improve from 1.74533\n",
      "Epoch 328/500\n",
      "1136924/1136924 [==============================] - 89s 78us/step - loss: 1.7464 - accuracy: 0.5307\n",
      "\n",
      "Epoch 00328: loss did not improve from 1.74533\n",
      "Epoch 329/500\n",
      "1136924/1136924 [==============================] - 89s 78us/step - loss: 1.7465 - accuracy: 0.5305\n",
      "\n",
      "Epoch 00329: loss did not improve from 1.74533\n",
      "Epoch 330/500\n",
      "1136924/1136924 [==============================] - 89s 78us/step - loss: 1.7467 - accuracy: 0.5306\n",
      "\n",
      "Epoch 00330: loss did not improve from 1.74533\n",
      "Epoch 331/500\n",
      "1136924/1136924 [==============================] - 90s 79us/step - loss: 1.7467 - accuracy: 0.5306\n",
      "\n",
      "Epoch 00331: loss did not improve from 1.74533\n",
      "Epoch 332/500\n",
      "1136924/1136924 [==============================] - 89s 78us/step - loss: 1.7462 - accuracy: 0.5303\n",
      "\n",
      "Epoch 00332: loss did not improve from 1.74533\n",
      "Epoch 333/500\n",
      "1136924/1136924 [==============================] - 82s 72us/step - loss: 1.7463 - accuracy: 0.5307\n",
      "\n",
      "Epoch 00333: loss did not improve from 1.74533\n",
      "Epoch 334/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7462 - accuracy: 0.5304\n",
      "\n",
      "Epoch 00334: loss did not improve from 1.74533\n",
      "Epoch 335/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7462 - accuracy: 0.5305\n",
      "\n",
      "Epoch 00335: loss did not improve from 1.74533\n",
      "Epoch 336/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7464 - accuracy: 0.5307\n",
      "\n",
      "Epoch 00336: loss did not improve from 1.74533\n",
      "Epoch 337/500\n",
      "1136924/1136924 [==============================] - 81s 71us/step - loss: 1.7465 - accuracy: 0.5309\n",
      "\n",
      "Epoch 00337: loss did not improve from 1.74533\n",
      "Epoch 338/500\n",
      "1136924/1136924 [==============================] - 81s 71us/step - loss: 1.7468 - accuracy: 0.5303\n",
      "\n",
      "Epoch 00338: loss did not improve from 1.74533\n",
      "Epoch 339/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7466 - accuracy: 0.5306\n",
      "\n",
      "Epoch 00339: loss did not improve from 1.74533\n",
      "Epoch 340/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7460 - accuracy: 0.5307\n",
      "\n",
      "Epoch 00340: loss did not improve from 1.74533\n",
      "Epoch 341/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7462 - accuracy: 0.5309\n",
      "\n",
      "Epoch 00341: loss did not improve from 1.74533\n",
      "Epoch 342/500\n",
      "1136924/1136924 [==============================] - 81s 71us/step - loss: 1.7469 - accuracy: 0.5306\n",
      "\n",
      "Epoch 00342: loss did not improve from 1.74533\n",
      "Epoch 343/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7465 - accuracy: 0.5311\n",
      "\n",
      "Epoch 00343: loss did not improve from 1.74533\n",
      "Epoch 344/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7465 - accuracy: 0.5306\n",
      "\n",
      "Epoch 00344: loss did not improve from 1.74533\n",
      "Epoch 345/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7469 - accuracy: 0.5303\n",
      "\n",
      "Epoch 00345: loss did not improve from 1.74533\n",
      "Epoch 346/500\n",
      "1136924/1136924 [==============================] - 80s 71us/step - loss: 1.7462 - accuracy: 0.5305\n",
      "\n",
      "Epoch 00346: loss did not improve from 1.74533\n",
      "Epoch 347/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7467 - accuracy: 0.5307\n",
      "\n",
      "Epoch 00347: loss did not improve from 1.74533\n",
      "Epoch 348/500\n",
      "1136924/1136924 [==============================] - 81s 71us/step - loss: 1.7468 - accuracy: 0.5303\n",
      "\n",
      "Epoch 00348: loss did not improve from 1.74533\n",
      "Epoch 349/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7465 - accuracy: 0.5307\n",
      "\n",
      "Epoch 00349: loss did not improve from 1.74533\n",
      "Epoch 350/500\n",
      "1136924/1136924 [==============================] - 2507s 2ms/step - loss: 1.7463 - accuracy: 0.5304\n",
      "\n",
      "Epoch 00350: loss did not improve from 1.74533\n",
      "Epoch 351/500\n",
      "1136924/1136924 [==============================] - 81s 71us/step - loss: 1.7469 - accuracy: 0.5306\n",
      "\n",
      "Epoch 00351: loss did not improve from 1.74533\n",
      "Epoch 352/500\n",
      "1136924/1136924 [==============================] - 80s 70us/step - loss: 1.7459 - accuracy: 0.5309\n",
      "\n",
      "Epoch 00352: loss did not improve from 1.74533\n",
      "Epoch 353/500\n",
      "1136924/1136924 [==============================] - 87s 76us/step - loss: 1.7470 - accuracy: 0.5303\n",
      "\n",
      "Epoch 00353: loss did not improve from 1.74533\n",
      "Epoch 354/500\n",
      "1136924/1136924 [==============================] - 83s 73us/step - loss: 1.7462 - accuracy: 0.5306\n",
      "\n",
      "Epoch 00354: loss did not improve from 1.74533\n",
      "Epoch 355/500\n",
      "1136924/1136924 [==============================] - 81s 71us/step - loss: 1.7465 - accuracy: 0.5306\n",
      "\n",
      "Epoch 00355: loss did not improve from 1.74533\n",
      "Epoch 356/500\n",
      "1136924/1136924 [==============================] - 79s 69us/step - loss: 1.7461 - accuracy: 0.5304\n",
      "\n",
      "Epoch 00356: loss did not improve from 1.74533\n",
      "Epoch 357/500\n",
      "1136924/1136924 [==============================] - 79s 69us/step - loss: 1.7469 - accuracy: 0.5307\n",
      "\n",
      "Epoch 00357: loss did not improve from 1.74533\n",
      "Epoch 358/500\n",
      " 883968/1136924 [======================>.......] - ETA: 18s - loss: 1.7448 - accuracy: 0.5307"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-162-bfb5b5261a45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'word_pred_Model4.h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3740\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3742\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \"\"\"\n\u001b[0;32m-> 1081\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1121\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "path = 'word_pred_Model4.h5'\n",
    "checkpoint = ModelCheckpoint(path, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.fit(X,Y,epochs=500,batch_size=128,verbose=1,use_multiprocessing=True,callbacks=[checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_similarity(a,b):\n",
    "    dot = np.dot(a, b)\n",
    "    norma = np.linalg.norm(a)\n",
    "    normb = np.linalg.norm(b)\n",
    "    cos = dot / (norma * normb)\n",
    "    \n",
    "    return cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def get_most_similar_chord(predicted):\n",
    "    greatest_similarity = 0\n",
    "    most_similar = None\n",
    "    \n",
    "    for chord in possible_chord_vectors.keys():\n",
    "        similarity = cos_similarity(predicted, possible_chord_vectors[chord])\n",
    "        \n",
    "        if similarity > greatest_similarity:\n",
    "            greatest_similarity = similarity\n",
    "            most_similar = chord\n",
    "    \n",
    "    print(f'Most similar: {chord}. Similarity: {greatest_similarity}')\n",
    "    \n",
    "    return most_similar\n",
    "\n",
    "def get_most_similar_chord_2(predicted):\n",
    "    distance_function = lambda a,b: np.linalg.norm(a-b)\n",
    "    lowest_distance = 10000000000\n",
    "    most_similar = None\n",
    "    \n",
    "    for chord in possible_chord_vectors.keys():\n",
    "        distance = distance_function(predicted, possible_chord_vectors[chord])\n",
    "        \n",
    "        if distance < lowest_distance:\n",
    "            lowest_distance = distance\n",
    "            most_similar = chord\n",
    "    \n",
    "    print(f'Most similar: {chord}. Distance: {lowest_distance}')\n",
    "    \n",
    "    return most_similar\n",
    "\n",
    "def get_most_similar_ranking(prediction_chord, distance_function):\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    similar_chords = pd.DataFrame(possible_chord_vectors.keys(), columns=['chord'])\n",
    "    similar_chords['similarity'] = similar_chords['chord'].apply(\\\n",
    "        lambda chord: distance_function(possible_chord_vectors[chord],prediction_chord[0]))\n",
    "    \n",
    "    return similar_chords.sort_values(by='similarity')\n",
    "\n",
    "def get_most_similar_chord(prediction_chord):\n",
    "    value = randint(0, 10)\n",
    "    chord = get_most_similar_ranking(prediction_chord,lambda a,b: np.linalg.norm(a-b) ).iloc[0]['chord']\n",
    "    print(chord)\n",
    "    return chord\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21.chord import Chord\n",
    "from music21.stream import Stream\n",
    "\n",
    "def show_sequence(chord_sequence):\n",
    "    parser = ChordParser()\n",
    "    stream = Stream()\n",
    "    \n",
    "    chord_sequence = [chord_sequence[0], *chord_sequence] # to solve a music21 problem\n",
    "\n",
    "    for chord_str in chord_sequence:\n",
    "        extended_chord = parser.parse(chord_str)\n",
    "        chord = Chord(notes=extended_chord.components, type='whole')\n",
    "        stream.append(chord)\n",
    "\n",
    "    stream.show()\n",
    "    stream.show('midi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "chord_str = ['C', 'G', 'C', 'D', 'G']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 471, 2, 136, 471]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch = [encoder.get_number_from_chord(chord) for chord in chord_str]\n",
    "ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(a, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    a = np.log(a) / temperature\n",
    "    a = np.exp(a) / np.sum(np.exp(a))\n",
    "    return np.argmax(np.random.multinomial(1, a, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.49345798e-08, 1.11169349e-04, 4.44702238e-01, 3.97179450e-04,\n",
       "       1.06328243e-05, 1.49802781e-05, 3.17733269e-04, 5.79644329e-05,\n",
       "       1.11192458e-04, 6.40410814e-04, 2.96807690e-10, 3.72812821e-08,\n",
       "       1.42549447e-06, 1.68834324e-09, 5.73312313e-08, 2.84445355e-06,\n",
       "       3.19488136e-05, 4.75193156e-05, 1.50218148e-06, 7.27504557e-10,\n",
       "       3.28399244e-12, 6.20864099e-04, 6.02492925e-12, 5.52952848e-03,\n",
       "       4.38876539e-11, 1.18524375e-04, 1.00109719e-06, 2.59786589e-07,\n",
       "       6.34094860e-12, 6.26559995e-06, 6.84690889e-12, 4.89855032e-12,\n",
       "       5.89729567e-12, 1.54068999e-04, 4.61916327e-09, 1.60827083e-04,\n",
       "       9.56226600e-08, 3.92351082e-12, 4.99628108e-08, 9.10250662e-07,\n",
       "       2.04261212e-08, 1.14961096e-11, 4.28413858e-12, 5.85180428e-12,\n",
       "       2.80488935e-12, 1.88587405e-12, 9.75922376e-07, 2.31272237e-10,\n",
       "       1.83641709e-12, 3.99403297e-12, 3.32128561e-11, 8.46166019e-12,\n",
       "       2.31205398e-12, 2.44524583e-12, 5.34839617e-10, 6.01977149e-12,\n",
       "       9.26035117e-12, 2.66553685e-12, 1.02844297e-07, 7.93550914e-09,\n",
       "       7.41906744e-11, 4.99147078e-12, 2.89664321e-12, 2.51748730e-12,\n",
       "       6.04677983e-12, 6.03016031e-12, 4.24312018e-12, 6.60528750e-12,\n",
       "       1.38261862e-06, 1.97312824e-04, 2.23055049e-05, 5.69830081e-05,\n",
       "       5.65102073e-06, 1.04826817e-07, 5.10028819e-08, 3.48213014e-08,\n",
       "       8.11371865e-06, 1.06653505e-11, 2.93560488e-12, 4.15700319e-09,\n",
       "       2.53957554e-12, 1.48747733e-11, 2.05581019e-09, 4.21206958e-11,\n",
       "       1.62092565e-05, 2.82432714e-13, 1.06697480e-05, 1.32681565e-11,\n",
       "       4.60119480e-07, 5.71000322e-12, 4.12074513e-07, 5.65313599e-07,\n",
       "       4.47145879e-12, 4.22685975e-12, 1.98854266e-09, 6.06748289e-12,\n",
       "       1.88297919e-10, 2.40355977e-12, 6.38731819e-12, 7.68383968e-12,\n",
       "       1.37619296e-07, 1.42513756e-09, 1.69691148e-07, 1.85547716e-10,\n",
       "       5.22848258e-12, 1.00852116e-09, 2.18423873e-11, 2.78690515e-10,\n",
       "       2.83544759e-12, 3.42589563e-12, 3.16734811e-12, 9.17650071e-12,\n",
       "       8.53985979e-12, 2.43642606e-10, 1.14039320e-10, 5.19275378e-12,\n",
       "       2.48871778e-12, 1.55579005e-09, 3.39471918e-12, 3.12702793e-12,\n",
       "       5.73447323e-12, 2.89013796e-11, 3.82767732e-12, 3.29534945e-12,\n",
       "       3.95870576e-12, 2.23648114e-10, 1.97570554e-11, 3.24133407e-12,\n",
       "       4.96386179e-12, 8.93823210e-12, 5.34690087e-12, 3.14476830e-12,\n",
       "       4.61691015e-12, 1.06422631e-08, 3.85192152e-07, 5.01791306e-04,\n",
       "       1.06205203e-01, 7.91691884e-04, 1.76858593e-05, 2.66638544e-05,\n",
       "       2.98626837e-04, 2.46858015e-03, 9.87861422e-05, 4.56299447e-03,\n",
       "       2.14541912e-10, 9.58149045e-08, 3.04197292e-05, 1.59252508e-07,\n",
       "       1.18445569e-05, 1.04079380e-04, 1.33019994e-05, 6.23395666e-04,\n",
       "       2.42441420e-05, 2.45489855e-06, 5.91316536e-12, 3.13146484e-05,\n",
       "       3.37620643e-12, 4.95124659e-05, 8.73323245e-07, 1.88145907e-06,\n",
       "       1.72742712e-05, 9.98294272e-05, 7.39349467e-12, 7.65516234e-06,\n",
       "       4.87096301e-12, 4.26361247e-12, 2.58657565e-07, 9.02422325e-05,\n",
       "       2.57598076e-05, 4.33429659e-06, 2.02208277e-08, 3.04592159e-12,\n",
       "       4.02106616e-06, 7.42207220e-08, 1.13822622e-07, 5.18373655e-09,\n",
       "       1.10240523e-11, 6.23728543e-12, 3.95958179e-12, 3.83424629e-12,\n",
       "       2.16512799e-05, 4.93269973e-11, 1.80713002e-07, 3.36524471e-12,\n",
       "       8.04233139e-07, 3.81868582e-12, 2.04362113e-12, 2.82829077e-12,\n",
       "       9.65183267e-10, 7.24508009e-10, 2.82945065e-12, 5.03482325e-12,\n",
       "       7.29812768e-07, 1.30458151e-08, 6.77528953e-12, 5.95293086e-12,\n",
       "       1.35705006e-11, 6.73736414e-12, 2.49852504e-12, 3.23318369e-12,\n",
       "       3.45859886e-12, 6.56463165e-12, 8.28082102e-06, 2.71860597e-04,\n",
       "       1.60201980e-05, 1.24086919e-05, 7.86251846e-08, 1.28955179e-07,\n",
       "       4.56639597e-08, 5.31281103e-06, 9.88584725e-06, 1.10443732e-09,\n",
       "       2.77284619e-12, 9.37702346e-11, 5.20460324e-12, 4.70954222e-12,\n",
       "       1.55105078e-11, 2.22264980e-08, 5.58790725e-06, 1.28626179e-10,\n",
       "       4.72617900e-09, 6.10261537e-12, 1.36612834e-05, 4.21872476e-12,\n",
       "       9.46527166e-07, 2.56265907e-12, 1.11875309e-12, 4.51635561e-12,\n",
       "       5.78419127e-12, 7.73039532e-12, 3.64422426e-09, 5.07008454e-12,\n",
       "       2.51486223e-12, 2.88395826e-12, 9.59065574e-07, 1.99436272e-08,\n",
       "       1.24076976e-07, 8.46944303e-12, 5.83847033e-12, 2.85977058e-11,\n",
       "       8.58570853e-11, 3.04178105e-09, 3.05189402e-12, 6.41762771e-12,\n",
       "       5.14991695e-12, 2.08645731e-12, 4.22270986e-12, 7.59133467e-09,\n",
       "       6.42767903e-14, 2.13706114e-12, 7.22958412e-12, 7.58749397e-10,\n",
       "       3.96614989e-12, 5.07715831e-12, 4.61141889e-12, 3.40725191e-12,\n",
       "       4.55683364e-12, 4.33208460e-12, 3.80478505e-12, 6.94803415e-09,\n",
       "       2.54423002e-12, 3.52682683e-12, 3.76271106e-12, 1.53784993e-12,\n",
       "       3.42391003e-12, 3.50006851e-12, 3.06437059e-12, 1.03987331e-11,\n",
       "       2.91771460e-08, 1.60927812e-04, 1.68452784e-03, 7.02166483e-02,\n",
       "       1.30797316e-05, 2.84693524e-05, 2.08369893e-05, 6.46393964e-05,\n",
       "       1.44673777e-06, 7.91463885e-04, 9.32158239e-11, 6.15937479e-09,\n",
       "       2.60988850e-06, 1.87035804e-10, 1.38066127e-07, 1.77710372e-05,\n",
       "       4.42076271e-05, 4.91350982e-03, 1.22267102e-05, 1.18318042e-06,\n",
       "       2.34209809e-12, 9.34499326e-07, 4.75985223e-12, 3.87607815e-06,\n",
       "       5.00485367e-05, 3.12419637e-08, 3.54631450e-12, 1.80493771e-05,\n",
       "       1.67700489e-12, 3.90664994e-08, 5.36598587e-12, 6.00763623e-12,\n",
       "       3.39554817e-12, 6.89489480e-06, 2.03881820e-04, 1.34944833e-06,\n",
       "       2.22108909e-09, 3.95954363e-12, 4.79530536e-06, 4.41818374e-05,\n",
       "       1.07537840e-10, 4.76619352e-12, 7.34166894e-12, 3.35946613e-12,\n",
       "       6.99664285e-12, 3.26469450e-12, 8.52282767e-07, 1.14895983e-11,\n",
       "       5.28790558e-08, 7.65382462e-12, 6.89836725e-06, 5.15107618e-12,\n",
       "       5.57198905e-12, 7.62618094e-12, 2.00633443e-09, 4.18830639e-12,\n",
       "       6.27772227e-12, 5.80923374e-12, 1.63320706e-07, 2.13138643e-12,\n",
       "       1.13354772e-05, 6.16421931e-12, 1.43564535e-11, 4.46616572e-12,\n",
       "       5.90872360e-12, 4.00263546e-12, 8.80846613e-08, 1.52916735e-10,\n",
       "       1.85154004e-05, 1.45654902e-02, 2.39750516e-04, 6.87198462e-06,\n",
       "       2.18581135e-06, 6.47009438e-05, 2.26042139e-05, 9.15572746e-05,\n",
       "       1.40167569e-04, 1.15752366e-10, 7.50578053e-08, 1.03669208e-05,\n",
       "       7.78200842e-07, 5.64225104e-07, 1.78734922e-07, 1.68296774e-05,\n",
       "       5.96960672e-06, 7.05801084e-10, 2.95746734e-08, 3.21013463e-12,\n",
       "       1.84187564e-04, 1.79920488e-12, 5.83280344e-05, 2.65915016e-11,\n",
       "       4.34193362e-06, 4.82146101e-10, 1.34596718e-07, 2.81399903e-12,\n",
       "       1.23597908e-06, 4.18844213e-12, 3.94774968e-12, 7.59842016e-12,\n",
       "       1.26953264e-05, 3.02954248e-08, 3.15942043e-06, 3.40640405e-10,\n",
       "       3.12991517e-07, 1.27737271e-13, 2.62839056e-11, 7.91426800e-08,\n",
       "       6.53991444e-11, 1.40867168e-12, 3.78477197e-12, 4.95014273e-12,\n",
       "       1.92365759e-11, 2.75315637e-09, 9.83119480e-06, 5.00598174e-12,\n",
       "       6.86131534e-12, 6.96311015e-15, 8.01363143e-12, 6.23638164e-12,\n",
       "       6.21625234e-12, 6.66200905e-12, 7.02217841e-12, 4.08432142e-12,\n",
       "       4.32498438e-12, 4.12476948e-06, 1.95136063e-09, 4.68446679e-12,\n",
       "       9.78718408e-13, 5.26238905e-12, 3.94841234e-12, 4.10663170e-12,\n",
       "       4.78418130e-12, 6.90693032e-12, 6.24945625e-12, 1.11233767e-05,\n",
       "       1.15973991e-03, 2.16607179e-04, 8.76300837e-05, 1.24198332e-05,\n",
       "       3.19421389e-07, 9.25090376e-07, 8.97584371e-08, 5.27809389e-05,\n",
       "       1.68273814e-07, 2.65222475e-08, 8.39098163e-07, 1.00852781e-11,\n",
       "       2.40342685e-12, 1.91480035e-06, 3.47266734e-07, 8.69288269e-05,\n",
       "       1.95038670e-08, 2.92610384e-05, 5.82934612e-12, 2.14570903e-07,\n",
       "       9.84258161e-12, 1.03586233e-08, 7.03241270e-08, 1.29199372e-13,\n",
       "       4.43890454e-12, 2.87157018e-08, 4.94248219e-12, 2.26589677e-07,\n",
       "       1.08474280e-11, 4.78210136e-12, 4.59992808e-12, 4.24888469e-07,\n",
       "       6.43868570e-10, 6.50652102e-11, 2.23532009e-11, 5.36646682e-12,\n",
       "       1.51927857e-08, 6.65024800e-08, 2.34076734e-12, 2.51778980e-12,\n",
       "       2.76164053e-12, 6.22488996e-12, 4.41029548e-12, 7.31978627e-12,\n",
       "       3.31219958e-08, 7.18867717e-12, 9.00179273e-08, 3.83048888e-12,\n",
       "       6.68599895e-08, 2.27465963e-12, 4.85510200e-12, 6.00019253e-12,\n",
       "       1.25654886e-11, 4.92887849e-12, 6.05284963e-12, 8.00306175e-12,\n",
       "       1.64742187e-09, 1.03039166e-09, 6.28793241e-12, 2.85877116e-12,\n",
       "       7.03605490e-12, 3.05748742e-12, 2.77279848e-12, 3.36672769e-12,\n",
       "       6.35957736e-12, 3.31429983e-05, 4.71589650e-04, 2.61789858e-01,\n",
       "       2.36627428e-04, 7.85672455e-06, 1.98916878e-05, 8.98518483e-05,\n",
       "       1.44401903e-03, 4.75017820e-04, 3.06596607e-03, 2.13037765e-09,\n",
       "       2.89728961e-12, 5.28531132e-07, 6.14417445e-07, 1.81697078e-06,\n",
       "       2.27763067e-05, 9.29774069e-06, 3.31537776e-05, 3.89649443e-07,\n",
       "       6.37514441e-09, 3.63779514e-12, 2.70227698e-04, 4.43299867e-12,\n",
       "       2.24042669e-04, 8.17310990e-07, 2.55684085e-07, 3.34595802e-06,\n",
       "       2.07841877e-04, 4.23098969e-12, 9.94413654e-07, 3.07264543e-07,\n",
       "       3.57407420e-12, 3.55277982e-12, 2.84250800e-05, 1.18702985e-06,\n",
       "       1.11172894e-05, 8.82851514e-10, 6.96211144e-12, 2.22654080e-06,\n",
       "       1.05016784e-06, 3.49336532e-12, 9.33910761e-12, 4.46263165e-12,\n",
       "       3.83554776e-12, 6.46370674e-12, 3.17537489e-12, 6.10764400e-05,\n",
       "       2.78050955e-12, 2.25311284e-12, 3.68366795e-12, 9.18947975e-08,\n",
       "       4.72264849e-12, 5.10867043e-12, 5.78157704e-12, 7.68796383e-07,\n",
       "       2.51320472e-07, 1.85947503e-12, 6.52116555e-12, 1.46428092e-05,\n",
       "       1.31819093e-06, 9.69754832e-12, 5.36904679e-12, 3.05596139e-11,\n",
       "       4.61338042e-12, 5.56223080e-12, 4.25823266e-12, 9.15991394e-10,\n",
       "       3.57365136e-12, 8.68668280e-07, 3.35801305e-04, 1.69443902e-05,\n",
       "       3.46205961e-05, 1.48766339e-05, 9.94948124e-10, 6.25221119e-08,\n",
       "       1.01745877e-06, 2.77503955e-06, 6.58250190e-11, 1.58046042e-11,\n",
       "       3.80602483e-09, 7.95833885e-12, 7.83813986e-12, 6.74261591e-10,\n",
       "       3.73918780e-08, 1.87058652e-06, 2.14858665e-12, 1.61503431e-05,\n",
       "       7.18126296e-12, 6.18870956e-07, 4.89903647e-12, 6.12560980e-09,\n",
       "       3.19419968e-12, 9.90885360e-11, 5.19256556e-12, 7.63947039e-09,\n",
       "       5.08675437e-12, 5.79614978e-09, 2.72145067e-12, 5.70508267e-12,\n",
       "       3.11012305e-12, 6.35380371e-09, 2.45683407e-09, 1.54841935e-08,\n",
       "       5.86372328e-09, 2.59058812e-12, 2.72720960e-11, 3.93237803e-10,\n",
       "       3.83622864e-12, 4.77074109e-12, 2.97226046e-12, 2.51952409e-12,\n",
       "       8.18368116e-12, 7.03664514e-12, 7.26341298e-08, 2.72516876e-09,\n",
       "       5.23715576e-12, 3.86336059e-12, 5.95545302e-08, 2.57585121e-12,\n",
       "       1.21324496e-11, 3.19753403e-12, 4.49689730e-11, 5.42084650e-12,\n",
       "       3.64129343e-12, 4.87712691e-12, 2.30588881e-08, 9.56860979e-11,\n",
       "       3.81889746e-12, 5.76082758e-12, 2.56118477e-11, 8.44353927e-12,\n",
       "       1.48212484e-11, 5.80457080e-12, 4.80437218e-12, 5.62562386e-10,\n",
       "       3.40667117e-04, 9.96227283e-03, 3.33513245e-02, 2.55545401e-05,\n",
       "       5.70393058e-06, 1.94729815e-04, 2.62229092e-04, 5.30653188e-06,\n",
       "       2.43000919e-03, 4.37379697e-08, 3.03329029e-11, 4.08521828e-06,\n",
       "       8.02983374e-12, 5.83466999e-06, 2.57745647e-04, 3.21696789e-05,\n",
       "       6.05299603e-03, 4.47325510e-06, 7.82224379e-06, 3.48791287e-12,\n",
       "       1.25892038e-05, 1.00276584e-11, 8.42026111e-06, 3.41223276e-05,\n",
       "       4.64340415e-12, 5.66774418e-07, 2.06486569e-04, 2.27036402e-12,\n",
       "       6.27558894e-09, 4.42376691e-12, 4.29248130e-12, 3.70143716e-12,\n",
       "       1.98279358e-05, 6.34832904e-05, 2.88481004e-07, 1.04048947e-09,\n",
       "       3.05166420e-10, 1.51012614e-06, 4.88010254e-09, 1.34514035e-17,\n",
       "       1.00205316e-10, 2.74865569e-12, 4.44936449e-12, 2.75598403e-12,\n",
       "       2.29981832e-12, 7.41068470e-06, 2.73033614e-12, 3.36454375e-09,\n",
       "       1.45089583e-11, 2.01858311e-05, 3.11013498e-12, 8.45461028e-12,\n",
       "       3.37226124e-12, 5.84515931e-07, 3.70563302e-12, 5.99949474e-12,\n",
       "       3.04725234e-12, 2.62801763e-07, 2.05655035e-10, 5.02745934e-06,\n",
       "       5.00919341e-11, 4.73126789e-12, 3.05814055e-12, 3.42039179e-12,\n",
       "       4.71927185e-12, 1.54886919e-07, 4.27379790e-12, 1.65040674e-05,\n",
       "       1.15767575e-03, 1.23545586e-04, 3.03599409e-05, 7.36950881e-07,\n",
       "       1.76837002e-05, 2.12442433e-06, 2.21922096e-06, 1.70345575e-05,\n",
       "       6.07506578e-09, 2.31213590e-10, 1.25277751e-07, 3.80956334e-12,\n",
       "       1.38060123e-08, 3.67969932e-07, 1.27116081e-07, 8.74759917e-06,\n",
       "       2.94378562e-12, 8.98031316e-10, 4.64630981e-12, 8.26994437e-06,\n",
       "       2.80012667e-12, 1.66662190e-07, 5.97160429e-12, 1.32747632e-11,\n",
       "       6.42030916e-12, 1.48711565e-09, 5.69529797e-12, 3.19688205e-08,\n",
       "       3.04264686e-12, 4.03929494e-12, 5.37702998e-12, 5.13723762e-06,\n",
       "       4.16934309e-09, 4.94893513e-08, 8.00338181e-12, 4.60802035e-10,\n",
       "       1.93710683e-08, 2.95087010e-11, 2.18004148e-09, 4.02936105e-12,\n",
       "       3.84831316e-12, 5.10252561e-12, 8.04010244e-12, 8.66939767e-12,\n",
       "       1.70006544e-07, 1.10633325e-08, 4.09541455e-12, 5.01537614e-12,\n",
       "       8.77420359e-11, 4.00284189e-12, 7.54740367e-12, 6.16924177e-12,\n",
       "       4.35694991e-13, 4.51489150e-12, 3.64887851e-12, 4.23595534e-12,\n",
       "       1.42751134e-07, 9.93437332e-09, 5.47658143e-12, 3.22119306e-12,\n",
       "       7.16416163e-12, 8.16690292e-12, 6.08870983e-12, 3.06406680e-12,\n",
       "       4.32157088e-12, 5.10131867e-12, 4.81645438e-05, 1.86899968e-03,\n",
       "       7.72215566e-03, 1.15714911e-05, 7.72198200e-06, 1.95207194e-05,\n",
       "       8.53879465e-06, 2.12384180e-07, 7.33065826e-04, 1.95475156e-08,\n",
       "       9.67595015e-12, 1.71276697e-05, 4.49269398e-12, 5.29794786e-09,\n",
       "       1.79627623e-05, 6.76969103e-06, 5.01997361e-04, 8.41889558e-10,\n",
       "       3.17024978e-06, 7.52990552e-12, 2.94244018e-07, 6.24123669e-12,\n",
       "       8.72095279e-06, 6.15615136e-07, 3.55236153e-11, 4.33457263e-12,\n",
       "       3.11992864e-07, 4.24014253e-12, 1.06096364e-11, 5.44673889e-11,\n",
       "       5.13458677e-12, 5.32499912e-12, 2.83771072e-07, 1.77145066e-05,\n",
       "       1.06666294e-08, 1.38920399e-07, 5.22873194e-12, 5.22216439e-08,\n",
       "       2.27471972e-07, 4.79847455e-12, 5.17661521e-12, 3.75426600e-12,\n",
       "       4.29145001e-12, 4.28825074e-12, 1.54560436e-12, 4.69629003e-06,\n",
       "       6.43838455e-12, 3.76255160e-09, 2.92111647e-12, 2.52243603e-06,\n",
       "       4.33474610e-12, 3.93445953e-12, 2.34825766e-12, 1.38942982e-08,\n",
       "       6.77373269e-10, 5.13439118e-12, 3.34622868e-12, 6.26416141e-11,\n",
       "       4.58272005e-12, 5.00767448e-11, 3.02084659e-12, 2.47406002e-12,\n",
       "       2.93469784e-12, 2.91224315e-12, 6.28704467e-12, 2.19335907e-05],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model.predict(np.array(np.array([ch])))[0]\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "471"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(prediction, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Am', 'E', 'A', 'Dm', 'E']\n",
      "['E', 'A', 'Dm', 'E', 'D']\n",
      "['A', 'Dm', 'E', 'D', 'G']\n",
      "['Dm', 'E', 'D', 'G', 'G']\n",
      "['E', 'D', 'G', 'G', 'Am']\n",
      "['D', 'G', 'G', 'Am', 'C']\n",
      "['G', 'G', 'Am', 'C', 'C']\n",
      "['G', 'Am', 'C', 'C', 'G']\n",
      "['Am', 'C', 'C', 'G', 'C']\n",
      "['C', 'C', 'G', 'C', 'G']\n",
      "['C', 'G', 'C', 'G', 'G']\n",
      "['G', 'C', 'G', 'G', 'C']\n",
      "['C', 'G', 'G', 'C', 'C']\n",
      "['G', 'G', 'C', 'C', 'D']\n",
      "['G', 'C', 'C', 'D', 'C']\n",
      "['C', 'C', 'D', 'C', 'D']\n",
      "['C', 'D', 'C', 'D', 'G']\n",
      "['D', 'C', 'D', 'G', 'C']\n",
      "['C', 'D', 'G', 'C', 'C']\n",
      "['D', 'G', 'C', 'C', 'Em']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 187,
       "width": 752
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <div id='midiPlayerDiv31949'></div>\n",
       "                <link rel=\"stylesheet\" href=\"//cuthbertLab.github.io/music21j/css/m21.css\"\n",
       "                    type=\"text/css\" />\n",
       "                <script>\n",
       "                require.config({\n",
       "                    paths: {'music21': '//cuthbertLab.github.io/music21j/src/music21'}\n",
       "                });\n",
       "                require(['music21'], function() {\n",
       "                               mp = new music21.miditools.MidiPlayer();\n",
       "                               mp.addPlayer('#midiPlayerDiv31949');\n",
       "                               mp.base64Load('data:audio/midi;base64,TVRoZAAAAAYAAQABBABNVHJrAAAClwD/AwAA4ABAAJBFWgCQPFoAkEBaoACARQAAgDwAAIBAAACQRVoAkDxaAJBAWqAAgEUAAIA8AACAQAAAkEBaAJBEWgCQR1qgAIBAAACARAAAgEcAAJBFWgCQPVoAkEBaoACARQAAgD0AAIBAAACQPloAkEFaAJBFWqAAgD4AAIBBAACARQAAkEBaAJBEWgCQR1qgAIBAAACARAAAgEcAAJA+WgCQQloAkEVaoACAPgAAgEIAAIBFAACQQ1oAkEdaAJA+WqAAgEMAAIBHAACAPgAAkENaAJBHWgCQPlqgAIBDAACARwAAgD4AAJBFWgCQPFoAkEBaoACARQAAgDwAAIBAAACQPFoAkEBaAJBDWqAAgDwAAIBAAACAQwAAkDxaAJBAWgCQQ1qgAIA8AACAQAAAgEMAAJBDWgCQR1oAkD5aoACAQwAAgEcAAIA+AACQPFoAkEBaAJBDWqAAgDwAAIBAAACAQwAAkENaAJBHWgCQPlqgAIBDAACARwAAgD4AAJBDWgCQR1oAkD5aoACAQwAAgEcAAIA+AACQPFoAkEBaAJBDWqAAgDwAAIBAAACAQwAAkDxaAJBAWgCQQ1qgAIA8AACAQAAAgEMAAJA+WgCQQloAkEVaoACAPgAAgEIAAIBFAACQPFoAkEBaAJBDWqAAgDwAAIBAAACAQwAAkD5aAJBCWgCQRVqgAIA+AACAQgAAgEUAAJBDWgCQR1oAkD5aoACAQwAAgEcAAIA+AACQPFoAkEBaAJBDWqAAgDwAAIBAAACAQwAAkDxaAJBAWgCQQ1qgAIA8AACAQAAAgEMAAJBAWgCQQ1oAkEdaoACAQAAAgEMAAIBHAACQQ1oAkEdaAJA+WqAAgEMAAIBHAACAPgCIAP8vAA==');\n",
       "                        });\n",
       "                </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ch_str = ['Am', 'E', 'A', 'Dm', 'E']\n",
    "\n",
    "for i in range(0,20):\n",
    "    last_five = ch_str[-5:]\n",
    "    print(last_five)\n",
    "    ch = [encoder.get_number_from_chord(chord) for chord in chord_str]\n",
    "    pre = model.predict(np.array(np.array([ch])))[0]\n",
    "    chord_index = sample(pre)\n",
    "    ch_str.append(encoder.get_chord_from_number(chord_index))\n",
    "\n",
    "show_sequence(ch_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
